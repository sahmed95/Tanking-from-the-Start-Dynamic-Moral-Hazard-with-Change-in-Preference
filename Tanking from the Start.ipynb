{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e4631f0",
   "metadata": {},
   "source": [
    "### Introduction \n",
    "\n",
    "The Jupyter Notebook contains the code to calculate effort levels as they vary with the probability of change in preference. The notebook also calculates how the rent to the agent and the profit to the principal changes with this probability and compares it with the repeated one shot contract and the contract that the principal offers after the state has been realized. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff90128f",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "I am focusing on a dynamic moral hazard problem where the principal has a change in preference in the second period. I want to model a situation where the principal values success and failure differently in the second period. Spefically, I model a situation where the difference between success and failure shrinks in the second period from the principal's point of view. I consider the effect this change in preference has on first-period induced effort even though the two periods are technologically independent. \n",
    "\n",
    "The benchmark case with risk neutrality and continuous effort demonstrates the \"hot hand\" effect. Even though a success in the first period has no technological effect whatsoever on the likelihood of a success in the second period, the principal implements $e_2^c(1) > e^{SB} > e_2^c(0)$. Giving the agent in the second period particularly high incentives following a first-period success (and particularly low incentives following a failure) has desirable spillover effects on the first period incentives: the agent works hard in the first period not only in order to get the direct reward $t_1(1)$, but also in order to enjoy a higher second period rent. Since, giving the agent incentives in the first period is now cheaper than in the one shot problem, the principal implements $e_1 > e^{SB}$.\n",
    "\n",
    "When the principal's difference in payoff from success and failure in the second period decreases with probability $p$, they will want to lower the second period incentives. This means that the power of using second period incentives for first period effort might be diminished. The effect on first period effort will vary based on whether the $p$ is independent/dependent on first period output and how we model the change in preference. When $p$ is independent of first period output and the principal loses from winning in the second period, effort in the first period decreases with $p$. However, if $p$ is dependent on first period output and is only relevant after a failure, effort in the first period increases to decrease the chance of having this change in preference. This conclusion is reversed if we model the change in preference as the principal having something to gain from losing in the second period with an output dependent $p$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781275ab",
   "metadata": {},
   "source": [
    "### Benchmark Case \n",
    "\n",
    "#### Optimization problem \n",
    "\n",
    "\n",
    "\n",
    "The incentives provided in the second period act as carrot and stick for the first period. The second period incentives can be used to partially circumvent the limited liability constraint where the agent cannot be punished. The prospect of a higher second period-rent following a first period success motivates the agent to exert more effort in the first period, that is, rents in the second period act as reward and punishment for the first period. A first period success will be indirectly rewarded by the prospect of getting a larger bonus for a second-period success if it follows a first-period success. In the one-shot interaction, the most severe punishment available to the principal is not to pay anything to the agent. The principal implements a second period effort level smaller than $e^{SB}$ when the first-period is a failure. The resulting smaller second-period rent acted as an indirect punishment of the wealth-constrained agent for the first-period failure. The following summarizes the effort levels in the benchmark problem:\n",
    "\n",
    "$$\n",
    "e^{FB} \\geq e_2^c(1) > e_1^c > e^{SB} > e_2^c(0) > 0\n",
    "$$\n",
    "\n",
    "Refer to the file \"Optimization Problem\" **Section 2.1** for the proof. \n",
    "\n",
    "**Question**: Why does the principal wait until the second period? The principal is supposed to give you a rent to work hard in the first-period but does not give them a rent and instead waits till the second period. This adds risk on the agent as a result need pay more in the second period so bigger bonus in second period.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748db7a1",
   "metadata": {},
   "source": [
    "#### Example: \n",
    "\n",
    "The following is for the cost function $c(e) = e^2$. The principal maximizes: \n",
    "\n",
    "$$e_1(1-t_1(1)+e_2(1)(1-t_2(1,1)))+(1-e_1)(-t_1(0)+e_2(0)(1-t_2(0,1)))$$\n",
    "\n",
    "subject to \n",
    "\n",
    "\\begin{align*}\n",
    "    t_1(1) \\geq 0 \\qquad &(LL_1) \\\\\n",
    "    t_1(0) \\geq 0 \\qquad &(LL_0) \\\\\n",
    "    t_1(1)+A(e_2(1)) = a(1) \\qquad & (5) \\\\\n",
    "    t_1(0) + A(e_2(0)) = a(0) \\qquad & (6)\\\\\n",
    "    a(1) - a(0) = c'(e_1) \\qquad &(IC_1) \\\\\n",
    "    t_2(y_1,1) = c'(e_2(y_1)) \\qquad & (IC_2)\n",
    "\\end{align*}\n",
    "\n",
    "The problem can be reduced to: \n",
    "\n",
    "$$\\max_{e_1, e_2(0), e_2(1), t_1(0)} e_1-2e_1^2+e_1(e_2(1)-e_2(1)^2)+(1-e_1)(e_2(0)-e_2(0)^2)-t_1(0)-e_2^2(0) $$ \n",
    "\n",
    "subject to \n",
    "\n",
    "$$t_1(0) \\geq 0$$ \n",
    "\n",
    "The Lagrangian is then given by: \n",
    "\n",
    "$$L = e_1-2e_1^2+e_1(e_2(1)-e_2(1)^2)+(1-e_1)(e_2(0)-e_2(0)^2)-t_1(0)-e_2^2(0)+\\lambda(t_1(0))$$\n",
    "\n",
    "\n",
    "The FOCs that characterize the optimal solution is given by: \n",
    "\n",
    "$$ [e_1]: \\ 1-4e_1+e_2(1)-e_2(1)^2-e_2(0)+e_2(0)^2= 0 $$\n",
    "\n",
    "$$ [e_2(0)]: \\ (1-e_1)-2(1-e_1)e_2(0)-2e_2(0) =0 $$\n",
    "\n",
    "$$ [e_2(1)]: \\ e_1+2e_1e_2(1) = 0 \\implies e_2(1) = \\frac{1}{2} = e^{FB} $$\n",
    "\n",
    "$$ [t_1(0)]: \\ -1+\\lambda = 0 \\implies \\lambda = 1 \\implies t_1(0) = 0 $$\n",
    "\n",
    "Solving for the optimal induced effort levels, we get: $e_1 = 0.2711, e_2(0) = 0.211$ and $e_2(1) = e^{FB} = 0.5$. The result for a specific is consistent with what we found under a general cost function for the benchmark problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76e4eb9f",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize \n",
    "from sympy import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(rc={\"figure.dpi\":300, 'savefig.dpi':300})\n",
    "sns.set_context('notebook')\n",
    "sns.set_style(\"ticks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03057eb",
   "metadata": {
    "hidePrompt": false
   },
   "source": [
    "#### Useful functions \n",
    "\n",
    "The following section contains a list of functions that will be used in the analysis later. \n",
    "\n",
    "\\begin{align*}\n",
    "c(e) &= e^2 \\\\\n",
    "c'(e) &= 2e \\\\\n",
    "\\text{Rent to the agent for effort} \\ e :  A(e) &= ec'(e) - c(e)\\\\\n",
    "\\text{Transfer to the agent in the first-period}: t_1(e_1, e_2(0), e_2(1)) &= c'(e_1)-A(e_2(1))+A(e_2(0)) \\\\\n",
    "\\text{Transfer to the agent in the second-period}: t_2(e_2) &= c'(e_2) \\\\\n",
    "\\text{First-best in the benchmark problem}: e^{FB} &= 0.5 \\\\\n",
    "\\text{Second-best in the becnhmark problem}: e^{SB} &= 0.25\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70f8d7a4",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Cost functions \n",
    "\n",
    "def c(e):\n",
    "    c = e**2\n",
    "    return c\n",
    "\n",
    "def c_e(e):\n",
    "    c = 2*e\n",
    "    return c\n",
    "\n",
    "# Rent to the agent \n",
    "\n",
    "def A(e):\n",
    "    val = e*c_e(e)-c(e)\n",
    "    return val\n",
    "\n",
    "# Transfer in the first period \n",
    "\n",
    "def t_1(e1, e20, e21):\n",
    "    t = c_e(e1)-A(e21)+A(e20)\n",
    "    return t\n",
    "\n",
    "# Transfer in the second period\n",
    "\n",
    "def t_2(e):\n",
    "    t = c_e(e)\n",
    "    return t\n",
    "\n",
    "# Total rent to the agent\n",
    "\n",
    "def total_A(e1, e20):\n",
    "    t_A = A(e1)+A(e20)\n",
    "    return t_A\n",
    "\n",
    "# Probability grid \n",
    "p_grid = np.linspace(0, 1, 495)\n",
    "p_grid_2 = np.linspace(0, 0.5, 495)\n",
    "\n",
    "# Some effort levels \n",
    "\n",
    "# First best \n",
    "\n",
    "e_fb = 0.5 \n",
    "\n",
    "# Second best \n",
    "\n",
    "e_sb = 0.25 \n",
    "\n",
    "# First best in static one-shot of new problem \n",
    "\n",
    "ep_fb = (1-2*p_grid_2)/2\n",
    "\n",
    "# Second best in static one-shot of new problem \n",
    "\n",
    "ep_sb = (1-2*p_grid_2)/4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a43201",
   "metadata": {},
   "source": [
    "### Probability is independent of first period output \n",
    "\n",
    "#### 1. Payoff of -1 after success \n",
    "\n",
    "The principal maximizes: \n",
    "\n",
    "$$ e_1(1-t_1(1)+e_2(1)(1-2p-t_2(1,1)))+(1-e_1)(-t_1(0)+e_2(0)(1-2p-t_2(0,1))) $$\n",
    "\n",
    "subject to \n",
    "\n",
    "\\begin{align*}\n",
    "    t_1(1) \\geq 0 \\qquad &(LL_1) \\\\\n",
    "    t_1(0) \\geq 0 \\qquad &(LL_0) \\\\\n",
    "    t_1(1)+A(e_2(1)) = a(1) \\qquad & \\\\\n",
    "    t_1(0) + A(e_2(0)) = a(0) \\qquad &\\\\\n",
    "    a(1) - a(0) = c'(e_1) \\qquad &(IC_1) \\\\\n",
    "    t_2(y_1,1) = c'(e_2(y_1)) \\qquad & (IC_2)\n",
    "\\end{align*}\n",
    "\n",
    "The problem reduces to the following: \n",
    "\n",
    "$$ \\max_{e_1, e_2(0), e_2(1), t_1(0)} e_1-e_1e_2(1)^2+e_1e_2(1)(1-2p)+(1-e_1)(e_2(0)(1-2p)-e_2(0)^2)-2e_1^2-t_1(0)-e_2(0)^2$$\n",
    "\n",
    "subject to \n",
    "\n",
    "$$t_1(0) \\geq 0$$\n",
    "\n",
    "The Lagrangian is then given by: \n",
    "\n",
    "$$L = e_1-e_1e_2(1)^2+e_1e_2(1)(1-2p)+(1-e_1)(e_2(0)(1-2p)-e_2(0)^2)-2e_1^2-t_1(0)-e_2(0)^2 +\\lambda (t_1(0))$$\n",
    "\n",
    "The FOCs are given by: \n",
    "\n",
    "\\begin{align*}\n",
    "[e_2(1)] :\\ &-2e_2(1)e_1+e_1(1-2p) = 0 \\implies e_2(1) = \\frac{1-2p}{2} \\\\\n",
    "\\\\\n",
    "[e_2(0)] :\\  &(1-e_1)(1-2p) -2e_2(0)(1-e_1)-2e_2(0) = 0\\\\\n",
    "\\\\\n",
    "[e_1] :\\ &1-e_2(1)^2 +e_2(1)(1-2p)-e_2(0)(1-2p)+e_2(0)^2-4e_1 = 0 \\\\\n",
    "\\\\ \n",
    "& \\implies 1-\\left(\\frac{1-2p}{2}\\right)^2+\\frac{(1-2p)^2}{2}-e_2(0)(1-2p)+e_2(0)^2-4e_1 = 0\\\\\n",
    "[t_1(0)]: \\ &-1+\\lambda = 0 \\implies \\lambda = 1 \\implies t_1(0) = 0 \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d74c12f5",
   "metadata": {
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "# Solves for the effort level given a value of p \n",
    "\n",
    "def f(p):\n",
    "    \n",
    "    # solves for effort levels e_1 and e_20 as a function of p\n",
    "    # a = e_1, b = e_20\n",
    "    \n",
    "    var('a b p')    \n",
    "    eqns = [1+(1-2*p)/2*(1-2*p)-((1-2*p)/2)**2-4*a-b*(1-2*p)+b**2, (1-a)*(1-2*p)-(4-2*a)*b]\n",
    "    soln = solve(eqns, [a,b])\n",
    "    return soln"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a8c10c",
   "metadata": {
    "hideCode": false,
    "hideOutput": false,
    "hidePrompt": false
   },
   "source": [
    "##### 1 (a): Effort levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4d6c2a",
   "metadata": {
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "e1_grid = list()\n",
    "e20_grid = list()\n",
    "for i in range(len(p_grid_2)):\n",
    "    \n",
    "        # No unique solution to the simultaneous equations\n",
    "        # We pick the real solutions between 0 and 1 below \n",
    "        \n",
    "        e1_grid.append(abs(f(p_grid_2[i])[0][0]))\n",
    "        e20_grid.append(abs(f(p_grid_2[i])[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c84dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the effort levels \n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.style.use('seaborn-white')\n",
    "plt.plot(p_grid_2, e1_grid, label = \"e_1D\")\n",
    "plt.plot(p_grid_2, e20_grid, label = \"e_2D(0)\")\n",
    "plt.plot(p_grid_2, ep_fb, label = \"e_2D(1)\")\n",
    "plt.plot(p_grid_2, [e_fb]*len(p_grid_2), '--', label = \"e_fb\")\n",
    "plt.plot(p_grid_2, [e_sb]*len(p_grid_2), '--',label = \"e_sb\")\n",
    "# plt.plot(p_grid_2, ep_sb, label = \"e_sb^p\")\n",
    "# plt.plot(p_grid_2, (1-p_grid)*0.25, label= \"e_2_realized\")\n",
    "# plt.axvline(x=0.245, color='b', ls = '--')\n",
    "plt.xticks([0, 0.10, 0.20, 0.30, 0.40, 0.50])\n",
    "plt.yticks([0, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50])\n",
    "plt.legend(loc=3, prop={'size': 16})\n",
    "plt.xlabel('Probability of change in preference (p)', fontname = 'Arial', fontsize = 14)\n",
    "plt.ylabel('Effort level', fontname='Arial', fontsize = 14)\n",
    "# plt.title('Effort levels vs p ', fontname='Helvetica', fontsize = 22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85128db4",
   "metadata": {},
   "source": [
    "All effort levels are decreasing in $p$. Even though the first period preference of the principal does not change, the change in pereference in period 2 affects first period preferences. Effort in period 2 decreases linearly following both a success and a failure. \n",
    "\n",
    "Effort in period 1 is bounded below by the second-best level of effort. The principal might not want to implement as high an effort level in period 2 (even after a success) because of this change in preference. At $p = \\frac{1}{2}$, the principal is indifferent between winning and losing in the second period and the principal can achieve that by setting $t_2(y_1)$ = 0 which gets rid of added first period incentives altogether. The principal is forced to use the first period to provide incentives for the first period effort and therefore implements the second best. The higher the $p$, the lower the difference in payoff from winning and losing from the principal in the second period. In a static problem, the principal would induce lower effort to reduce the chances of success which might lead to a negative payoff. The principal still induces the static first-best of this 'new' problem in period 2 after a success but the first-best falls with a slope of 1 with respect to $p$. The effort in period 2 following a failure decreases slower (**why**) than after a success and thus the ''carrot'' is shrinking faster than the ''stick'' is getting larger. As a result, effort in the first-period is decreasing. \n",
    "\n",
    "A team that has a 50-50 chance of not making the playoffs they want to induce the second-best effort level in the first-period and induce no effort in period 2. In the second-period, the principal is indifferent between winning and losing and so the principal will simply pay the agent nothing and induce zero effort. Moreover, since first period effort has no bite on the second period expected payoff, the principal simply wants to induce the second best level as if they were solving a static problem. The second period is irrelevant because the principal has no incentive to win and therefore to induce any effort from the agent. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbcdf33",
   "metadata": {},
   "source": [
    "#####  1 (b):  Rent to the agent \n",
    "\n",
    "We will compare the rent to the agent when 1) long-term contract is offered, 2) one-shot contract with an interim IR is offered, 3) one-period contract with no interim IR is offered and 4) one-period contract after the state is realized is offered. \n",
    "\n",
    "1. Long-term Contract: $A(e_1)+A(e_2(0))$\n",
    "2. One-shot contract with an interim IR: $A(e_{sb})+A(e^p_{sb})$ where $e^p_sb$ is the second best from the new one-shot problem\n",
    "3. One-shot contract without an interim IR: \n",
    "4. One-period contract after the state is realized: $A(e_{sb})+(1-p)A(e_{sb})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115d2a76",
   "metadata": {
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "# Comparison of rents \n",
    "\n",
    "# 1. Long-term solution rent \n",
    "long_grid = list()\n",
    "\n",
    "# 2. One-period contract contract with interim IR\n",
    "repeated_IR_grid = list()\n",
    "\n",
    "# 4. One-period contract after the state is realized\n",
    "realized_grid = list()\n",
    "\n",
    "\n",
    "for i in range(len(p_grid_2)):\n",
    "    e1 = e1_grid[i]\n",
    "    e20 = e20_grid[i]\n",
    "    e21 = ep_fb[i]\n",
    "    p = p_grid_2[i]\n",
    "    e_sb_p = ep_sb[i]\n",
    "    # 1. Long-term solution \n",
    "    long_grid.append(A(e1)+A(e20))\n",
    "    # 2. One-period contract with interim IR\n",
    "    repeated_IR_grid.append(A(e_sb)+A(e_sb_p))\n",
    "    # 4. One-period contract after state is realized\n",
    "    realized_grid.append(A(e_sb)+(1-p)*A(e_sb))\n",
    "    \n",
    "\n",
    "# Plotting the rents \n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.style.use('seaborn-white')\n",
    "plt.plot(p_grid_2, long_grid, label = \"Long-term contract\")\n",
    "plt.plot(p_grid_2, repeated_IR_grid, label = \"One-period contract with an interim IR\")\n",
    "plt.plot(p_grid_2, realized_grid, label = \"One-period contract with state realized\")\n",
    "# plt.plot(p_grid, [0.5]*len(p_grid), label = \"e_fb\")\n",
    "# plt.plot(p_grid, [0.25]*len(p_grid), label = \"e_sb\")\n",
    "# plt.plot(p_grid, (1-2*p_grid)/4, label = \"e_sb^p\")\n",
    "# plt.axvline(x=0.245, color='b', ls = '--')\n",
    "plt.xticks([0, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50])\n",
    "# plt.yticks([0, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50])\n",
    "plt.legend(loc=3, prop={'size': 12})\n",
    "# plt.grid()\n",
    "plt.xlabel('Probability of change in preference (p)', fontname = 'Arial', fontsize = 14)\n",
    "plt.ylabel('Rent to the agent', fontname='Arial', fontsize = 14)\n",
    "# plt.title('Rent to the agent(Probability independent of output)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f7f62c",
   "metadata": {
    "hideCode": false
   },
   "source": [
    "In the long-term contract, the agent’s expected rent is:\n",
    "            $$A(e_1) + A(e_2(0))$$\n",
    "This is despite the fact that the agent receives an additional rent of $A(e^{FB})$ in the second period following a success as this rent is extracted from the agent in the first period.\n",
    "\n",
    "If the one shot contract is repeated then the principal induces $e^{SB}$ in the first period and then $e^{SB}_p = \\frac{1-2p}{4}$ the second period and simply plays the marginal cost of effort in each period. The expected rent of the agent is given by:\n",
    "$$A(e^{SB}) + A(e^{SB}_p)$$\n",
    "\n",
    "If the principal waits for the realization of the state before offering a contract, then in the second period they will either induce $e^{SB}$ or zero. They induce $e^{SB}$ with a probability of $(1-p)$. Hence, the expected rent of the agent is:\n",
    "$$A(e^{SB}) + (1-p)A(e^{SB})$$\n",
    "\n",
    "The agent is worse off in the long-term contract for all feasible values of $p$. The agent prefers the\n",
    "contract where the principal waits for the realization of the state of nature. Rent to the agent is strictly increasing in effort. Expected effort in the contract where the principal\n",
    "waits for the state to be realized is strictly greater than $e^{SB}_p$ and so the rent to the agent in this type of contract will be strictly higher than rent in the repeated one-shot contract.\n",
    "\n",
    "The induced effort level in the first period in the long-term contract is higher than the induced effort level in the other types of contract. However, the effort level following a failure in the long-term contract is smaller than $e^{SB}_\n",
    "p$ and the expected effort level in the contract where the principal waits for the realization of the state. The difference between the second period effort levels after a failure is greater than difference between the first period effort levels and since the agent’s rent in the long-term contract only depends on the effort level induced after a failure, the agent is worse off in the long-term contract.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bf5d29",
   "metadata": {},
   "source": [
    "##### 1 (c): Profit to the principal \n",
    "\n",
    "We will compare the profit to the principal when 1) long-term contract is offered, 2) one-shot contract with an interim IR is offered, 3) one-period contract with no interim IR is offered and 4) one-period contract after the state is realized is offered. \n",
    "\n",
    "1. Long-term Contract: $$e_1(1-t_1(1)+e_2(1)(1-2p-t_2(1,1)))+(1-e_1)(e_2(0)(1-2p-t_2(0,1)))$$\n",
    "\n",
    "2. One-shot contract with an interim IR: $$e^{SB}(1-c'(e^{SB}))+e_p^{SB}(1-2p-c'(e_p^{SB}))$$\n",
    "\n",
    "3. One-shot contract without an interim IR: \n",
    "\n",
    "4. One-period contract after the state is realized: $$e^{SB}(1-c'(e^{SB}))+(1-p)(e^{SB}(1-c'(e^{SB})))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a881b7c7",
   "metadata": {
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "# Comparison of profits \n",
    "\n",
    "# Long-term contract\n",
    "profit_long = list()\n",
    "\n",
    "# Repeated contract \n",
    "profit_repeated_IR = list()\n",
    "\n",
    "# Realization contract\n",
    "profit_realized = list()\n",
    "\n",
    "\n",
    "for i in range(len(p_grid_2)):\n",
    "    e1 = e1_grid[i]\n",
    "    e20 = e20_grid[i]\n",
    "    e21 = ep_fb[i]\n",
    "    p = p_grid_2[i]\n",
    "    ep_sb_p = ep_sb[i]\n",
    "    # 1. Long-term solution\n",
    "    profit_long.append(e1*(1-t_1(e1,e20, e21)+e21*(1-2*p-t_2(e21)))+(1-e1)*(e20*(1-2*p-t_2(e20))))\n",
    "    # 2. One-period contract after state is realized\n",
    "    profit_repeated_IR.append(e_sb*(1-c_e(e_sb))+ep_sb_p*(1-2*p-c_e(ep_sb_p)))\n",
    "    # 4. One-period contract after state is realized\n",
    "    profit_realized.append(e_sb*(1-c_e(e_sb))+(1-p)*(e_sb*(1-c_e(e_sb))))\n",
    "    \n",
    "    \n",
    "# Plotting the profit to the principal for the different types of contracts\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.style.use('seaborn-white')\n",
    "plt.plot(p_grid_2, profit_long, label = \"Long-term contract\")\n",
    "plt.plot(p_grid_2, profit_repeated_IR, label = \"One-period contract with an interim IR\")\n",
    "plt.plot(p_grid_2, profit_realized, label = \"One-period contract with state realized\")\n",
    "# plt.plot(p_grid, [0.5]*len(p_grid), label = \"e_fb\")\n",
    "# plt.plot(p_grid, [0.25]*len(p_grid), label = \"e_sb\")\n",
    "# plt.plot(p_grid, (1-2*p_grid)/4, label = \"e_sb^p\")\n",
    "# plt.axvline(x=0.245, color='b', ls = '--')\n",
    "plt.xticks([0, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50])\n",
    "plt.legend(loc=3, prop={'size': 12})\n",
    "# plt.grid()\n",
    "plt.xlabel('Probability of change in preference (p)', fontname = 'Arial', fontsize = 14)\n",
    "plt.ylabel('Profit for the Principal', fontname='Arial', fontsize = 14)\n",
    "# plt.title('Profit for the Principal (Probability independent of output)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57189492",
   "metadata": {},
   "source": [
    "The profit to the principal under the different types of contracts is decreasing in $p$. The profit from the long-term contract does strictly better than the one-shot repeated contract with an interim IR except for when $p = \\frac{1}{2}$. For $p = \\frac{1}{2}$, no effort is induced in the second period regardless of first period-failure or success and $e^{SB}$ is induced in the first-period. The same is true for the repeated one-shot contract as no effort is induced in the second-period. The long-term contract does better than the realized contract only for small values of $p$. After a certain cutoff value for $p$, the realzied contract does better. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef944a7",
   "metadata": {
    "hideCode": false
   },
   "source": [
    "\n",
    "#### 2. Payoff of 0 after success\n",
    "\n",
    "The principal maximizes: \n",
    "\n",
    "$$ e_1(1-t_1(1)+e_2(1)(1-p-t_2(1,1)))+(1-e_1)(-t_1(0)+e_2(0)(1-p-t_2(0,1))) $$\n",
    "\n",
    "subject to \n",
    "\n",
    "\\begin{align*}\n",
    "    t_1(1) \\geq 0 \\qquad &(LL_1) \\\\\n",
    "    t_1(0) \\geq 0 \\qquad &(LL_0) \\\\\n",
    "    t_1(1)+A(e_2(1)) = a(1) \\qquad & \\\\\n",
    "    t_1(0) + A(e_2(0)) = a(0) \\qquad &\\\\\n",
    "    a(1) - a(0) = c'(e_1) \\qquad &(IC_1) \\\\\n",
    "    t_2(y_1,1) = c'(e_2(y_1)) \\qquad & (IC_2)\n",
    "\\end{align*}\n",
    "\n",
    "The problem reduces to the following: \n",
    "\n",
    "$$ \\max_{e_1, e_2(0), e_2(1), t_1(0)} e_1-e_1e_2(1)^2+e_1e_2(1)(1-p)+(1-e_1)(e_2(0)(1-p)-e_2(0)^2)-2e_1^2-t_1(0)-e_2(0)^2$$\n",
    "\n",
    "subject to \n",
    "\n",
    "$$t_1(0) \\geq 0$$\n",
    "\n",
    "The Lagrangian is then given by: \n",
    "\n",
    "$$L = e_1-e_1e_2(1)^2+e_1e_2(1)(1-p)+(1-e_1)(e_2(0)(1-p)-e_2(0)^2)-2e_1^2-t_1(0)-e_2(0)^2 +\\lambda (t_1(0))$$\n",
    "\n",
    "The FOCs are given by: \n",
    "\n",
    "\\begin{align*}\n",
    "[e_2(1)] :\\ &-2e_2(1)e_1+e_1(1-p) = 0 \\implies e_2(1) = \\frac{1-p}{2} \\\\\n",
    "\\\\\n",
    "[e_2(0)] :\\  &(1-e_1)(1-p) -2e_2(0)(1-e_1)-2e_2(0) = 0\\\\\n",
    "\\\\\n",
    "[e_1] :\\ &1-e_2(1)^2 +e_2(1)(1-p)-e_2(0)(1-p)+e_2(0)^2-4e_1 = 0 \\\\\n",
    "\\\\ \n",
    "& \\implies 1-\\left(\\frac{1-p}{2}\\right)^2+\\frac{(1-p)^2}{2}-e_2(0)(1-p)+e_2(0)^2-4e_1 = 0\\\\\n",
    "\\\\\n",
    "[t_1(0)]: \\ &-1+\\lambda = 0 \\implies \\lambda = 1 \\implies t_1(0) = 0 \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c38389",
   "metadata": {
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "# Solves for the effort level given a value of p \n",
    "\n",
    "def f_0(p):\n",
    "    \n",
    "    # solves for effort levels e_1 and e_20 as a function of p\n",
    "    # a = e_1, b = e_20\n",
    "    \n",
    "    var('a b p')\n",
    "    eqns = [1+(1-p)/2*(1-p)-((1-p)/2)**2-4*a-b*(1-p)+b**2, (1-a)*(1-p)-(4-2*a)*b]\n",
    "    soln = solve(eqns, [a,b])\n",
    "    return soln\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37a04b8",
   "metadata": {},
   "source": [
    "##### 2 (a): Effort levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66efef5",
   "metadata": {
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "e1_grid_0 = list()\n",
    "e20_grid_0 = list()\n",
    "\n",
    "# First best in static one-shot setting of new problem \n",
    "\n",
    "ep_fb_0 = (1-p_grid)/2\n",
    "\n",
    "# Second best in static one-shot setting of new problem  \n",
    "\n",
    "ep_sb_0 = (1-p_grid)/4\n",
    "\n",
    "for i in range(len(p_grid)):\n",
    "    \n",
    "        # No unique solution to the simultaneous equations\n",
    "        # We pick the real solutions between 0 and 1 below\n",
    "        \n",
    "        e1_grid_0.append(abs(f_0(p_grid[i])[0][0]))\n",
    "        e20_grid_0.append(abs(f_0(p_grid[i])[0][1]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ba4af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the effort levels \n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.style.use('seaborn-white')\n",
    "plt.plot(p_grid, e1_grid_0, label = \"e_1D\")\n",
    "plt.plot(p_grid, e20_grid_0, label = \"e_2D(0)\")\n",
    "plt.plot(p_grid, ep_fb_0, label = \"e_2D(1)\")\n",
    "plt.plot(p_grid, [e_fb]*len(p_grid), '--',label = \"Static e_fb\")\n",
    "plt.plot(p_grid, [e_sb]*len(p_grid), '--',label = \"Static e_sb\")\n",
    "# plt.plot(p_grid, ep_sb_0, label = \"e_sb^p\")\n",
    "# plt.plot(p_grid, (1-p_grid)*0.25, label= \"e_2_realized\")\n",
    "plt.xticks([0, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90,0.95,1.00])\n",
    "plt.yticks([0, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50])\n",
    "plt.legend(loc=3, prop={'size': 16})\n",
    "# plt.grid()\n",
    "plt.xlabel('Probability of change in preference (p)', fontname = 'Arial', fontsize = 14)\n",
    "plt.ylabel('Effort level', fontname='Arial', fontsize = 14)\n",
    "# plt.title('Effort levels vs p (Probability independent of output)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d7a80b",
   "metadata": {},
   "source": [
    "All effort levels are decreasing in $p$. Even though the first period preference of the principal does not change, the change in pereference in period 2 affects first period preferences. Effort in period 2 decreases linearly following both a success and a failure. \n",
    "\n",
    "Effort in period 1 is bounded below by the second-best level of effort. The principal might not want to implement as high an effort level in period 2 (even after a success) because of this change in preference. At $p = 1$, the principal is indifferent between winning and losing in the second period and the principal can achieve that by setting $t_2(y_1)$ = 0 which gets rid of added first period incentives altogether. The principal is forced to use the first period to provide incentives for the first period effort and therefore implements the second best. The higher the $p$, the lower the difference in payoff from winning and losing from the principal in the second period. In a static problem, the principal would induce lower effort because the principal would not want to pay as much to the agent and so the wedge between $t(1)$ and $t(0)$ will be smaller. The principal still induces the static first-best of this 'new' problem in period 2 after a success but the first-best falls with a slope of $\\frac{1}{2}$ with respect to $p$. The effort in period 2 following a failure decreases slower (**why**) than after a success and thus the ''carrot'' is shrinking faster than the ''stick'' is getting larger. As a result, effort in the first-period is decreasing. \n",
    "\n",
    "Let us consider a team that knows for sure that they will not make the playoffs regardless of whether they win or lose. This essentially becomes a one-period problem for the team. They will induce the second-best effort level in the first-period and induce no effort in period 2. In the second-period, the principal is indifferent between winning and losing and so the principal will simply pay the agent nothing and induce zero effort. Moreover, since first period effort has no bite on the second period expected payoff, the principal simply wants to induce the second best level as if they were solving a static problem. The second period is irrelevant because the principal has no incentive to win and therefore to induce any effort from the agent. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdbcc9c",
   "metadata": {},
   "source": [
    "##### 2 (b): Rent to the agent \n",
    "\n",
    "We will compare the rent to the agent when 1) long-term contract is offered, 2) one-shot contract with an interim IR is offered, 3) one-period contract with no interim IR is offered and 4) one-period contract after the state is realized is offered. \n",
    "\n",
    "1. Long-term Contract: $A(e_1)+A(e_2(0))$\n",
    "2. One-shot contract with an interim IR: $A(e_{sb})+A(e^p_{sb})$ where $e_p^{SB}$ is the second-best from the new one-shot problem\n",
    "3. One-shot contract without an interim IR: \n",
    "4. One-period contract after the state is realized: $A(e_{sb})+(1-p)A(e_{sb})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97c7748",
   "metadata": {
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "# Comparison of rents \n",
    "\n",
    "# 1. Long-term solution rent \n",
    "long_grid_0 = list()\n",
    "\n",
    "# 2. One-period contract contract with interim IR\n",
    "repeated_IR_grid_0 = list()\n",
    "\n",
    "# 4. One-period contract after the state is realized\n",
    "realized_grid_0 = list()\n",
    "\n",
    "\n",
    "for i in range(len(p_grid_2)):\n",
    "    e1 = e1_grid_0[i]\n",
    "    e20 = e20_grid_0[i]\n",
    "    e21 = ep_fb_0[i]\n",
    "    p = p_grid[i]\n",
    "    e_sb_p = ep_sb_0[i]\n",
    "    # 1. Long-term solution \n",
    "    long_grid_0.append(A(e1)+A(e20))\n",
    "    # 2. One-period contract with interim IR\n",
    "    repeated_IR_grid_0.append(A(e_sb)+A(e_sb_p))\n",
    "    # 4. One-period contract after state is realized\n",
    "    realized_grid_0.append(A(e_sb)+(1-p)*A(e_sb))\n",
    "    \n",
    "\n",
    "# Plotting the rents \n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.style.use('seaborn-white')\n",
    "plt.plot(p_grid, long_grid_0, label = \"Long-term contract\")\n",
    "plt.plot(p_grid, repeated_IR_grid_0, label = \"One-period contract with an interim IR\")\n",
    "plt.plot(p_grid, realized_grid_0, label = \"One-period contract with state realized\")\n",
    "# plt.plot(p_grid, [0.5]*len(p_grid), label = \"e_fb\")\n",
    "# plt.plot(p_grid, [0.25]*len(p_grid), label = \"e_sb\")\n",
    "# plt.plot(p_grid, (1-2*p_grid)/4, label = \"e_sb^p\")\n",
    "# plt.axvline(x=0.245, color='b', ls = '--')\n",
    "plt.xticks([0, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95, 1.00])\n",
    "# plt.yticks([0, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50])\n",
    "plt.legend(loc=3, prop={'size': 12})\n",
    "# plt.grid()\n",
    "plt.xlabel('Probability of change in preference (p)', fontname = 'Arial', fontsize = 14)\n",
    "plt.ylabel('Rent to the agent', fontname='Arial', fontsize = 14)\n",
    "# plt.title('Rent to the agent(Probability independent of output)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0341555",
   "metadata": {},
   "source": [
    "The rent to the agent is decreasing in $p$ for all types of contract. The agent is worse off in the long-term contract. The rent to the agent in the long-term contract depends on the second-period effort after a failure which is smaller than the expected second period effort in the other two contracts. The expected second period effort in the contract where the principal waits for the state to be realized is: $(1-p)e^{SB}$. The expected second period effort in the repeated contract with an interim IR is: $\\frac{1-p}{4}$. The expected second period effort is the same in both contracts but the rent is different. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809b7fd9",
   "metadata": {},
   "source": [
    "##### 2 (c):  Profit to the principal \n",
    "\n",
    "We will compare the profit to the principal when 1) long-term contract is offered, 2) one-shot contract with an interim IR is offered, 3) one-period contract with no interim IR is offered and 4) one-period contract after the state is realized is offered. \n",
    "\n",
    "1. Long-term Contract: $$e_1(1-t_1(1)+e_2(1)(1-p-t_2(1,1)))+(1-e_1)(e_2(0)(1-p-t_2(0,1)))$$\n",
    "\n",
    "2. One-shot contract with an interim IR: $$e^{SB}(1-c'(e^{SB}))+e_p^{SB}(1-p-c'(e_p^{SB}))$$\n",
    "\n",
    "3. One-shot contract without an interim IR: \n",
    "\n",
    "4. One-period contract after the state is realized: $$e^{SB}(1-c'(e^{SB}))+(1-p)(e^{SB}(1-c'(e^{SB})))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e9020e",
   "metadata": {
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "# Comparison of profits \n",
    "\n",
    "# Long-term contract\n",
    "profit_long_0 = list()\n",
    "\n",
    "# Repeated contract \n",
    "profit_repeated_IR_0 = list()\n",
    "\n",
    "# Realization contract\n",
    "profit_realized_0 = list()\n",
    "\n",
    "\n",
    "for i in range(len(p_grid)):\n",
    "    e1 = e1_grid_0[i]\n",
    "    e20 = e20_grid_0[i]\n",
    "    e21 = ep_fb_0[i]\n",
    "    p = p_grid[i]\n",
    "    ep_sb_p = ep_sb_0[i]\n",
    "    # 1. Long-term solution\n",
    "    profit_long_0.append(e1*(1-t_1(e1,e20, e21)+e21*(1-p-t_2(e21)))+(1-e1)*(e20*(1-p-t_2(e20))))\n",
    "    # 2. One-period contract after state is realized\n",
    "    profit_repeated_IR_0.append(e_sb*(1-c_e(e_sb))+ep_sb_p*(1-p-c_e(ep_sb_p)))\n",
    "    # 4. One-period contract after state is realized\n",
    "    profit_realized_0.append(e_sb*(1-c_e(e_sb))+(1-p)*(e_sb*(1-c_e(e_sb))))\n",
    "\n",
    "\n",
    "# Plotting the profit to the principal from different types of contracts\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.plot(p_grid, profit_long_0, label = \"long-term\")\n",
    "plt.plot(p_grid, profit_repeated_IR_0, label = \"repeated_IR\")\n",
    "plt.plot(p_grid, profit_realized_0, label = \"realization\")\n",
    "# plt.plot(p_grid, [0.5]*len(p_grid), label = \"e_fb\")\n",
    "# plt.plot(p_grid, [0.25]*len(p_grid), label = \"e_sb\")\n",
    "# plt.plot(p_grid, (1-2*p_grid)/4, label = \"e_sb^p\")\n",
    "# plt.axvline(x=0.245, color='b', ls = '--')\n",
    "plt.xticks([0, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50,0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90,0.95,1.00])\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel('Probability of change in preference (p)')\n",
    "plt.ylabel('Profit for the Principal')\n",
    "plt.title('Profit for the Principal (Probability independent of output)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e3a6a0",
   "metadata": {},
   "source": [
    "The profit to the principal under the different types of contracts is decreasing in $p$. The profit from the long-term contract does strictly better than the one-shot repeated contract with an interim IR except for when $p = 1$. For $p = 1$, no effort is induced in the second period regardless of first period-failure or success and $e^{SB}$ is induced in the first-period. The same is true for the repeated one-shot contract as no effort is induced in the second-period. The long-term contract does better than the realized contract only for small values of $p$. After a certain cutoff value for $p$, the realzied contract does better. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf04f042",
   "metadata": {},
   "source": [
    "#### 3. Payoff of 1 after failure\n",
    "\n",
    "The principal maximizes: \n",
    "\n",
    "$$ e_1(1-t_1(1)+e_2(1)(1-t_2(1,1))+(1-e_2(1))p)+(1-e_1)(-t_1(0)+e_2(0)(1-t_2(0,1))+(1-e_2(0))p) $$\n",
    "\n",
    "subject to \n",
    "\n",
    "\\begin{align*}\n",
    "    t_1(1) \\geq 0 \\qquad &(LL_1) \\\\\n",
    "    t_1(0) \\geq 0 \\qquad &(LL_0) \\\\\n",
    "    t_1(1)+A(e_2(1)) = a(1) \\qquad & \\\\\n",
    "    t_1(0) + A(e_2(0)) = a(0) \\qquad &\\\\\n",
    "    a(1) - a(0) = c'(e_1) \\qquad &(IC_1) \\\\\n",
    "    t_2(y_1,1) = c'(e_2(y_1)) \\qquad & (IC_2)\n",
    "\\end{align*}\n",
    "\n",
    "The problem reduces to the following: \n",
    "\n",
    "$$ \\max_{e_1, e_2(0), e_2(1), t_1(0)} e_1-e_1e_2(1)^2+e_1e_2(1)(1-p)+e_1p+(1-e_1)(e_2(0)(1-p)-e_2(0)^2+p)-2e_1^2-t_1(0)-e_2(0)^2$$\n",
    "\n",
    "subject to \n",
    "\n",
    "$$t_1(0) \\geq 0$$\n",
    "\n",
    "The Lagrangian is then given by: \n",
    "\n",
    "$$L = e_1-e_1e_2(1)^2+e_1e_2(1)(1-p)+e_1p+(1-e_1)(e_2(0)(1-p)-e_2(0)^2+p)-2e_1^2-t_1(0)-e_2(0)^2 +\\lambda (t_1(0))$$\n",
    "\n",
    "The FOCs are given by: \n",
    "\n",
    "\\begin{align*}\n",
    "[e_2(1)] :\\ &-2e_2(1)e_1+e_1(1-p) = 0 \\implies e_2(1) = \\frac{1-p}{2} \\\\\n",
    "\\\\\n",
    "[e_2(0)] :\\  &(1-e_1)(1-p) -2e_2(0)(1-e_1)-2e_2(0) = 0\\\\\n",
    "\\\\\n",
    "[e_1] :\\ &1-e_2(1)^2 +e_2(1)(1-p)-e_2(0)(1-p)+e_2(0)^2-4e_1 = 0 \\\\\n",
    "\\\\ \n",
    "& \\implies 1-\\left(\\frac{1-p}{2}\\right)^2+\\frac{(1-p)^2}{2}-e_2(0)(1-p)+e_2(0)^2-4e_1 = 0\\\\\n",
    "\\\\\n",
    "[t_1(0)]: \\ &-1+\\lambda = 0 \\implies \\lambda = 1 \\implies t_1(0) = 0\n",
    "\\end{align*}\n",
    "\n",
    "**Notice:** The FOCs are the same as in **2)** when the principal receives a payoff of 0 after success with an independent probability $p$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45060d58",
   "metadata": {
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "# Solves for the effort level given a value of p \n",
    "\n",
    "def f_1(p):\n",
    "    \n",
    "    # solves for effort levels e_1 and e_20 as a function of p\n",
    "    # a = e_1, b = e_20\n",
    "    \n",
    "    var('a b p')\n",
    "    eqns = [1+(1-p)*(1-p)/2-((1-p)/2)**2+b**2-(1-p)*b-4*a, (1-a)*(1-p)+(-4+2*a)*b]\n",
    "    soln = solve(eqns, [a,b])\n",
    "    return soln\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aed1319",
   "metadata": {},
   "source": [
    "##### 3 (a). Effort levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f4340e",
   "metadata": {
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "e1_grid_1 = list()\n",
    "e20_grid_1 = list()\n",
    "\n",
    "# First best in static setting of new problem \n",
    "\n",
    "ep_fb_0 = (1-p_grid)/2\n",
    "\n",
    "# Second best in static setting of new problem  \n",
    "\n",
    "ep_sb_0 = (1-p_grid)/4\n",
    "\n",
    "for i in range(len(p_grid)):\n",
    "    \n",
    "        # No unique solution to the simultaneous equations\n",
    "        # We pick the real solutions between 0 and 1 below\n",
    "        \n",
    "        e1_grid_1.append(abs(f_1(p_grid[i])[0][0]))\n",
    "        e20_grid_1.append(abs(f_1(p_grid[i])[0][1]))\n",
    "        \n",
    "# Plotting the effort levels \n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.plot(p_grid, e1_grid_1, label = \"e_1\")\n",
    "plt.plot(p_grid, e20_grid_1, label = \"e_20\")\n",
    "plt.plot(p_grid, ep_fb_0, label = \"e_21\")\n",
    "plt.plot(p_grid, [e_fb]*len(p_grid), label = \"e_fb\")\n",
    "plt.plot(p_grid, [e_sb]*len(p_grid), label = \"e_sb\")\n",
    "# plt.plot(p_grid, ep_sb_0, label = \"e_sb^p\")\n",
    "# plt.plot(p_grid, (1-p_grid)*0.25, label= \"e_2_realized\")\n",
    "plt.xticks([0, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90,0.95,1.00])\n",
    "plt.yticks([0, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50])\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel('Probability of change in preference (p)')\n",
    "plt.ylabel('Effort level')\n",
    "plt.title('Effort levels vs p (Probability independent of output)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038b6602",
   "metadata": {},
   "source": [
    "\n",
    "The induced effort levels are the same as in the case where the principal has a payoff of 0 after success. As $p$ increases, the gap between success and failure diminishes and the principal becomes more and more indifferent between winning and losing. This indifference does not depend on first-period output, that is, $p$ is independent of first-period output and so $e_2(0)$ and $e_2(1)$ both fall with $p$. For example, for $p = 1$ the principal is indifferent between winning and losing and so will not want to induce any effort in the second-period regardless of the outcome in the first-period. This is exactly what we saw in the case with a payoff of $0$ after success with probability $p$ as the carrot and stick are both shrinking but the carrot shrinks faster than the stick so effort in the first-period also decreases. This identical result boils down to the fact that the first-period effort/outcome has no bite on second period expected payoff. \n",
    "\n",
    "The second-best effort in the static one-shot version of this new problem is the same as the second-best effort in the static one-shot version of **2)**. As a result, the rent to the agent for different values of $p$ under all three different contracts is going to be the same as in the case with **2)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6fea5e",
   "metadata": {},
   "source": [
    "#####  3 (c):  Profit to the principal \n",
    "\n",
    "We will compare the profit to the principal when 1) long-term contract is offered, 2) one-shot contract with an interim IR is offered, 3) one-period contract with no interim IR is offered and 4) one-period contract after the state is realized is offered. \n",
    "\n",
    "1. Long-term Contract: $$e_1(1-t_1(1)+e_2(1)(1-t_2(1,1))+(1-e_2(1))p)+(1-e_1)(e_2(0)(1-t_2(0,1))+(1-e_2(0))p)$$\n",
    "\n",
    "2. One-shot contract with an interim IR: $$e^{SB}(1-c'(e^{SB}))+e_p^{SB}(1-c'(e_p^{SB}))+(1-e_p^{SB})(p)$$\n",
    "\n",
    "3. One-shot contract without an interim IR: \n",
    "\n",
    "4. One-period contract after the state is realized: $$e^{SB}(1-c'(e^{SB}))+(1-p)(e^{SB}(1-c'(e^{SB})))+p$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f512f5d2",
   "metadata": {
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "# Comparison of profits \n",
    "\n",
    "# Long-term contract\n",
    "profit_long_1 = list()\n",
    "\n",
    "# Repeated contract \n",
    "profit_repeated_IR_1 = list()\n",
    "\n",
    "# Realization contract\n",
    "profit_realized_1 = list()\n",
    "\n",
    "\n",
    "for i in range(len(p_grid)):\n",
    "    e1 = e1_grid_1[i]\n",
    "    e20 = e20_grid_1[i]\n",
    "    e21 = ep_fb_0[i]\n",
    "    p = p_grid[i]\n",
    "    ep_sb_p = ep_sb_0[i]\n",
    "    # 1. Long-term solution\n",
    "    profit_long_1.append(e1*(1-t_1(e1,e20, e21)+e21*(1-t_2(e21))+(1-e21)*p)+(1-e1)*(e20*(1-t_2(e20))+(1-e20)*p))\n",
    "    # 2. One-period contract after state is realized\n",
    "    profit_repeated_IR_1.append(e_sb*(1-c_e(e_sb))+ep_sb_p*(1-c_e(ep_sb_p))+(1-ep_sb_p)*p)\n",
    "    # 4. One-period contract after state is realized\n",
    "    profit_realized_1.append(e_sb*(1-c_e(e_sb))+(1-p)*(e_sb*(1-c_e(e_sb)))+p)\n",
    "\n",
    "\n",
    "# Plotting the profit to the principal from different types of contracts\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.plot(p_grid, profit_long_1, label = \"long-term\")\n",
    "plt.plot(p_grid, profit_repeated_IR_1, label = \"repeated_IR\")\n",
    "plt.plot(p_grid, profit_realized_1, label = \"realization\")\n",
    "# plt.plot(p_grid, [0.5]*len(p_grid), label = \"e_fb\")\n",
    "# plt.plot(p_grid, [0.25]*len(p_grid), label = \"e_sb\")\n",
    "# plt.plot(p_grid, (1-2*p_grid)/4, label = \"e_sb^p\")\n",
    "# plt.axvline(x=0.245, color='b', ls = '--')\n",
    "plt.xticks([0, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50,0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90,0.95,1.00])\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel('Probability of change in preference (p)')\n",
    "plt.ylabel('Profit for the Principal')\n",
    "plt.title('Profit for the Principal (Probability independent of output)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a601eb84",
   "metadata": {},
   "source": [
    "The profit to the principal is increasing in $p$. The higher the $p$ the higher the chance of getting a payoff of $1$ even after a failure. The long-term contract performs better than the repeated one-shot contract. The long-term contract also performs better than the contract that is offered after the state is realized for small values of $p$. For higher values of $p$, the principal should opt for the contract where they wait for the state to be realized. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325359fe",
   "metadata": {},
   "source": [
    "#### 4. Probability of change in preference is higher after failure (payoff of 0 after success)\n",
    "\n",
    "Let $p_L$ be the probability of change in preference after a success and $p_H$ be the probability of change in preference after a failure with $p_H > p_L$.\n",
    "\n",
    "The principal maximizes: \n",
    "\n",
    "$$ e_1(1-t_1(1)+e_2(1)(1-p_L-t_2(1,1)))+(1-e_1)(-t_1(0)+e_2(0)(1-p_H-t_2(0,1))) $$\n",
    "\n",
    "subject to \n",
    "\n",
    "\\begin{align*}\n",
    "    t_1(1) \\geq 0 \\qquad &(LL_1) \\\\\n",
    "    t_1(0) \\geq 0 \\qquad &(LL_0) \\\\\n",
    "    t_1(1)+A(e_2(1)) = a(1) \\qquad & \\\\\n",
    "    t_1(0) + A(e_2(0)) = a(0) \\qquad &\\\\\n",
    "    a(1) - a(0) = c'(e_1) \\qquad &(IC_1) \\\\\n",
    "    t_2(y_1,1) = c'(e_2(y_1)) \\qquad & (IC_2)\n",
    "\\end{align*}\n",
    "\n",
    "The problem reduces to the following: \n",
    "\n",
    "$$ \\max_{e_1, e_2(0), e_2(1), t_1(0)} e_1-e_1e_2(1)^2+e_1e_2(1)(1-p_L)+(1-e_1)(e_2(0)(1-p_H)-e_2(0)^2)-2e_1^2-t_1(0)-e_2(0)^2$$\n",
    "\n",
    "subject to \n",
    "\n",
    "$$t_1(0) \\geq 0$$\n",
    "\n",
    "The Lagrangian is then given by: \n",
    "\n",
    "$$L = e_1-e_1e_2(1)^2+e_1e_2(1)(1-p_L)+(1-e_1)(e_2(0)(1-p_H)-e_2(0)^2)-2e_1^2-t_1(0)-e_2(0)^2 +\\lambda (t_1(0))$$\n",
    "\n",
    "The FOCs are given by: \n",
    "\n",
    "\\begin{align*}\n",
    "[e_2(1)] :\\ &-2e_2(1)e_1+e_1(1-p_L) = 0 \\implies e_2(1) = \\frac{1-p_H}{2} \\\\\n",
    "\\\\\n",
    "[e_2(0)] :\\  &(1-e_1)(1-p_H) -2e_2(0)(1-e_1)-2e_2(0) = 0\\\\\n",
    "\\\\\n",
    "[e_1] :\\ &1-e_2(1)^2 +e_2(1)(1-p_L)-e_2(0)(1-p_H)+e_2(0)^2-4e_1 = 0 \\\\\n",
    "\\\\ \n",
    "& \\implies 1-\\left(\\frac{1-p}{2}\\right)^2+\\frac{(1-p)^2}{2}-e_2(0)(1-p)+e_2(0)^2-4e_1 = 0\\\\\n",
    "\\\\\n",
    "[t_1(0)]: \\ &-1+\\lambda = 0 \\implies \\lambda = 1 \\implies t_1(0) = 0 \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c897c2b8",
   "metadata": {},
   "source": [
    "##### 4(a): Effort levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea52973",
   "metadata": {
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "# Solves for the effort level given a value of p_H and p_L\n",
    "\n",
    "def f_q(p,q):\n",
    "    \n",
    "    # q = p_L, p = p_H\n",
    "    \n",
    "    if np.isnan(p):\n",
    "        return [[0,0]]\n",
    "    \n",
    "    # solves for effort levels e_1 and e_20 as a function of p\n",
    "    # a = e_1, b = e_20\n",
    "    \n",
    "    var('a b')\n",
    "    eqns = [1+(1-q)/2*(1-q)-((1-q)/2)**2-4*a-b*(1-p)+b**2, (1-a)*(1-p)-(4-2*a)*b]\n",
    "    soln = solve(eqns, [a,b])\n",
    "    return soln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d85a556",
   "metadata": {
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "p_grid = np.linspace(0,1, 25)\n",
    "q_grid = np.linspace(0,1, 25)\n",
    "soln_grid_1 = np.zeros((len(p_grid), len(q_grid)))\n",
    "for i in range(len(p_grid)):\n",
    "    p = p_grid[i]\n",
    "    for j in range((len(q_grid))):\n",
    "        q = q_grid[j]\n",
    "        if p > q: \n",
    "            val = abs(f_q(p,q)[0][0])\n",
    "            soln_grid_1[i,j] = val\n",
    "        else: \n",
    "            soln_grid_1[i,j] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d369a666",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plotting the effort levels \n",
    "plt.figure(figsize=(12,8))\n",
    "h = plt.contourf(p_grid,q_grid, soln_grid_1.T, levels= np.linspace(0,0.32,40))\n",
    "plt.axis('scaled')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Probability of change in preference after failure (p0)', fontname = 'Arial', fontsize = 14)\n",
    "plt.ylabel('Probability of change in preference after success (p1)', fontname = 'Arial', fontsize = 14)\n",
    "# plt.title('Effort levels in the first-period', fontsize = 25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742d9962",
   "metadata": {},
   "source": [
    "If we hold $p_H$ constant, then effort in the first period decreases with $p_L$. As $p_L$ increases, the 'carrot' is getting smaller and so inducing effort in the first period becomes costlier. As a result, the induced effort is reduced. Now, if we hold $p_L$ constant we can notice that the effort in the first-period increases with $p_H$. This is because the 'stick' is getting smaller, that is, the 'stick' is getting stronger and more effectie. As a result, inducing effort in the first-period becomes cheaper and therefore, higher first-period effort is induced. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d2aa4e",
   "metadata": {},
   "source": [
    "### Probability dependent on first period output (Change in preference only after failure )\n",
    "### General problem\n",
    "\n",
    "Let $\\alpha$ be the payoff from failure after change in prefernece and $\\beta$ be the payoff from success after change in preference. The principal maximizes \n",
    "\n",
    "$$ e_1(1-t_1(1)+e_2(1)(1-t_2(1,1))+(1-e_1)(-t_1(0)+e_2(0)(p\\beta-p\\alpha+1-p-t_2(0,1))+p\\alpha) $$\n",
    "\n",
    "subject to \n",
    "\n",
    "\\begin{align*}\n",
    "    t_1(1) \\geq 0 \\qquad &(LL_1) \\\\\n",
    "    t_1(0) \\geq 0 \\qquad &(LL_0) \\\\\n",
    "    t_1(1)+A(e_2(1)) = a(1) \\qquad & \\\\\n",
    "    t_1(0) + A(e_2(0)) = a(0) \\qquad &\\\\\n",
    "    a(1) - a(0) = c'(e_1) \\qquad &(IC_1) \\\\\n",
    "    t_2(y_1,1) = c'(e_2(y_1)) \\qquad & (IC_2)\n",
    "\\end{align*}\n",
    "\n",
    "The problem reduces to the following: \n",
    "\n",
    "$$ \\max_{e_1, e_2(0), e_2(1), t_1(0)} e_1(1-e_2(1)^2+e_2(1))+(1-e_1)(-e_2(0)^2+e_2(0)(p\\beta-p\\alpha+1-p)+p\\alpha)-2e_1^2-t_1(0)-e_2(0)^2$$\n",
    "\n",
    "subject to \n",
    "\n",
    "$$t_1(0) \\geq 0$$\n",
    "\n",
    "The Lagrangian is then given by: \n",
    "\n",
    "$$L = e_1(1-e_2(1)^2+e_2(1))+(1-e_1)(-e_2(0)^2+e_2(0)(p\\beta-p\\alpha+1-p)+p\\alpha)-2e_1^2-t_1(0)-e_2(0)^2 +\\lambda (t_1(0))$$\n",
    "\n",
    "The FOCs are given by: \n",
    "\n",
    "\\begin{align*}\n",
    "[e_2(1)] :\\ &-2e_2(1)e_1+e_1 = 0 \\implies e_2(1) = \\frac{1}{2} \\\\\n",
    "\\\\\n",
    "[e_2(0)] :\\  & -2(1-e_1)e_2(0)+(1-e_1)(p\\beta-p\\alpha+1-p)-2e_2(0) = 0\\\\\n",
    "\\\\\n",
    "[e_1] :\\ & (1+0.25)-4e_1+e_2(0)^2-e_2(0)(p\\beta-p\\alpha+1-p)-p\\alpha = 0 \\\\\n",
    "\\\\\n",
    "[t_1(0)]: \\ &-1+\\lambda = 0 \\implies \\lambda = 1 \\implies t_1(0) = 0\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402ae3f5",
   "metadata": {
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "# Solves for the effort level given a value of p \n",
    "\n",
    "\n",
    "def f_dep(alpha, beta, p):\n",
    "    \n",
    "    # solves for effort levels e_1 and e_20 as a function of p\n",
    "    # a = e_1, b = e_20\n",
    "    # alpha: payoff from failure after change in preference \n",
    "    # beta: payoff from success after change in preference \n",
    "    \n",
    "    var('a b p')\n",
    "    eqns = [1+0.25-4*a+b**2-b-b*p*beta+b*p-(1-b)*p*alpha, -2*b*(1-a)+(1-a)+(1-a)*p*beta-(1-a)*p-p*alpha+a*p*alpha-2*b]\n",
    "    soln = solve(eqns, [a,b])\n",
    "    return soln"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1ff240",
   "metadata": {},
   "source": [
    "#### 1. Payoff of 0 after success with probability p after failure in Period 1\n",
    "\n",
    "In this case, $\\alpha = 0$ and $\\beta = 0$. \n",
    "\n",
    "The principal maximizes \n",
    "\n",
    "$$ e_1(1-t_1(1)+e_2(1)(1-t_2(1,1))+(1-e_1)(-t_1(0)+e_2(0)(1-p-t_2(0,1))) $$\n",
    "\n",
    "subject to \n",
    "\n",
    "\\begin{align*}\n",
    "    t_1(1) \\geq 0 \\qquad &(LL_1) \\\\\n",
    "    t_1(0) \\geq 0 \\qquad &(LL_0) \\\\\n",
    "    t_1(1)+A(e_2(1)) = a(1) \\qquad & \\\\\n",
    "    t_1(0) + A(e_2(0)) = a(0) \\qquad &\\\\\n",
    "    a(1) - a(0) = c'(e_1) \\qquad &(IC_1) \\\\\n",
    "    t_2(y_1,1) = c'(e_2(y_1)) \\qquad & (IC_2)\n",
    "\\end{align*}\n",
    "\n",
    "The problem reduces to the following: \n",
    "\n",
    "$$ \\max_{e_1, e_2(0), e_2(1), t_1(0)} e_1(1-e_2(1)^2+e_2(1))+(1-e_1)(-e_2(0)^2+e_2(0)(1-p))-2e_1^2-t_1(0)-e_2(0)^2$$\n",
    "\n",
    "subject to \n",
    "\n",
    "$$t_1(0) \\geq 0$$\n",
    "\n",
    "The Lagrangian is then given by: \n",
    "\n",
    "$$L = e_1(1-e_2(1)^2+e_2(1))+(1-e_1)(-e_2(0)^2+e_2(0)(1-p))-2e_1^2-t_1(0)-e_2(0)^2 +\\lambda (t_1(0))$$\n",
    "\n",
    "The FOCs are given by: \n",
    "\n",
    "\\begin{align*}\n",
    "[e_2(1)] :\\ &-2e_2(1)e_1+e_1 = 0 \\implies e_2(1) = \\frac{1}{2} \\\\\n",
    "\\\\\n",
    "[e_2(0)] :\\  & -2(1-e_1)e_2(0)+(1-e_1)(1-p)-2e_2(0) = 0\\\\\n",
    "\\\\\n",
    "[e_1] :\\ & (1+0.25)-4e_1+e_2(0)^2-e_2(0)(1-p) = 0 \\\\\n",
    "\\\\\n",
    "[t_1(0)]: \\ &-1+\\lambda = 0 \\implies \\lambda = 1 \\implies t_1(0) = 0\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05871d30",
   "metadata": {},
   "source": [
    "##### 1 (a). Effort levels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2b0a97",
   "metadata": {
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "alpha = 0\n",
    "beta = 0\n",
    "\n",
    "\n",
    "e1_dep = list()\n",
    "e20_dep = list()\n",
    "for i in range(len(p_grid)):\n",
    "    \n",
    "        # No unique solution to the simultaneous equations\n",
    "        # We pick the real solutions between 0 and 1 below\n",
    "        \n",
    "        e1_dep.append(abs(f_dep(alpha, beta, p_grid[i])[0][0]))\n",
    "        e20_dep.append(abs(f_dep(alpha, beta, p_grid[i])[0][1]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e5b3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(e1_dep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2be9d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plotting the effort levels \n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.style.use('seaborn-white')\n",
    "plt.plot(p_grid, e1_dep, label = \"e_1D\")\n",
    "plt.plot(p_grid, e20_dep, label = \"e_2D(0)\")\n",
    "plt.plot(p_grid, [e_fb]*len(p_grid), label = \"e_2D(1)\")\n",
    "plt.plot(p_grid, [e_sb]*len(p_grid), '--', label = \"Static e_sb\")\n",
    "# plt.plot(p_grid, 0.25*0.25+(0.75)*(1-p_grid)*0.25, label = \"e2_realized\")\n",
    "# plt.axvline(x=0.245, color='b', ls = '--')\n",
    "plt.xticks([0, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90,0.95,1.00])\n",
    "plt.yticks([0, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50])\n",
    "plt.legend(loc=3, prop={'size': 16})\n",
    "plt.xlabel('Probability of change in preference (p)', fontname = 'Arial', fontsize = 14)\n",
    "plt.ylabel('Effort level', fontname='Arial', fontsize = 14)\n",
    "# plt.title('Effort levels vs p (Probability dependent on output)', fontsize = 25)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebf254b",
   "metadata": {},
   "source": [
    "######  1 (b). Rent to the agent \n",
    "\n",
    "We will compare the rent to the agent when 1) long-term contract is offered, 2) one-shot contract with an interim IR is offered, 3) one-period contract with no interim IR is offered and 4) one-period contract after the state is realized is offered. \n",
    "\n",
    "The one-period contract after the state is realized is going to implement second-best in the first-period. After a success in the first-period, the principal will implement $e^{SB}$ in the second-period as well and this occurs with a probability of $e_1$. After a failure, the principal will implement $e^{SB}$ if there is no change in preference and 0 otherwise. \n",
    "\n",
    "The one-shot contract with an interim IR will implement $e^{SB}$ in the first-period. In the second period, the principal will implement $e^{SB}$ after a success. After a failure, the principal will implement $e_p^{SB}$, the second-best effort in the new problem, in the second period. \n",
    "1. Long-term Contract: $A(e_1)+A(e_2(0))$\n",
    "2. One-shot contract with an interim IR: $A(e^{SB})+e^{SB}A(e^{SB})+(1-e^{SB})A(e^{SB}_p)$\n",
    "3. One-shot contract without an interim IR: \n",
    "4. One-period contract after the state is realized: $A(e^{SB})+(e^{SB}+(1-e^{SB})(1-p))A(e^{SB})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38efe00d",
   "metadata": {
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "# Comparison of rents \n",
    "\n",
    "# 1. Long-term solution rent \n",
    "long_grid = list()\n",
    "\n",
    "# 2. One-period contract contract with interim IR\n",
    "repeated_IR_grid = list()\n",
    "\n",
    "# 4. One-period contract after the state is realized\n",
    "realized_grid = list()\n",
    "\n",
    "\n",
    "for i in range(len(p_grid)):\n",
    "    e1= e1_dep[i]\n",
    "    e20 = e20_dep[i]\n",
    "    e21 = e_fb\n",
    "    p = p_grid[i]\n",
    "    e_sb_p = ep_sb[i]\n",
    "    # 1. Long-term solution \n",
    "    long_grid.append(A(e1)+A(e20))\n",
    "    # 2. One-period contract with interim IR\n",
    "    repeated_IR_grid.append(A(e_sb)+e_sb*A(e_sb)+(1-e_sb)*A(e_sb_p))\n",
    "    # 4. One-period contract after state is realized\n",
    "    realized_grid.append(A(e_sb)+(e_sb+(1-e_sb)*(1-p))*A(e_sb))\n",
    "\n",
    "# Plotting the rents \n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.style.use('seaborn-white')\n",
    "plt.plot(p_grid, long_grid, label = \"Long-term contract\")\n",
    "plt.plot(p_grid, repeated_IR_grid, label = \"One-period contract with an interim IR\")\n",
    "plt.plot(p_grid, realized_grid, label = \"One-period contract with state realized\")\n",
    "# plt.plot(p_grid, [0.5]*len(p_grid), label = \"e_fb\")\n",
    "# plt.plot(p_grid, [0.25]*len(p_grid), label = \"e_sb\")\n",
    "# plt.plot(p_grid, (1-2*p_grid)/4, label = \"e_sb^p\")\n",
    "# plt.axvline(x=0.245, color='b', ls = '--')\n",
    "plt.xticks([0, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95, 1.00])\n",
    "# plt.yticks([0, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50])\n",
    "plt.legend(loc=3, prop={'size': 12})\n",
    "# plt.grid()\n",
    "plt.xlabel('Probability of change in preference (p)', fontname = 'Arial', fontsize = 14)\n",
    "plt.ylabel('Rent to the agent', fontname='Arial', fontsize = 14)\n",
    "# plt.title('Rent to the agent(Probability dependent on output)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8f3e56",
   "metadata": {},
   "source": [
    "For higher values of $p$, the agent is better off in the long-term contract. The contract where the principal waits for the state to be realized is not always preferred by the agent. For any value of $p$, the agent always prefer some other contract to the one shot contract with repeated IR. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5956e9ec",
   "metadata": {},
   "source": [
    "##### c. Profit to the principal \n",
    "\n",
    "We will compare the profit to the principal when 1) long-term contract is offered, 2) one-shot contract with an interim IR is offered, 3) one-period contract with no interim IR is offered and 4) one-period contract after the state is realized is offered. \n",
    "\n",
    "\n",
    "1. Long-term Contract: $$e_1(1-t_1(1)+e_2(1)(1-t_2(1,1)))+(1-e_1)(e_2(0)(1-p-t_2(0,1))$$\n",
    "\n",
    "2. One-shot contract with an interim IR: $$e^{SB}(1-c'(e^{SB}))+e^{SB}(e^{SB}(1-c'(e^{SB})))+(1-e^{SB})(e_p^{SB}(1-p-c'(e_p^{SB}))$$\n",
    "\n",
    "3. One-shot contract without an interim IR: \n",
    "\n",
    "4. One-period contract after the state is realized: $$e^{SB}(1-c'(e^{SB}))+(e^{SB}+(1-e^{SB})(1-p))(e^{SB}(1-c'(e^{SB})))$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2fa946",
   "metadata": {
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "# Comparison of profits \n",
    "\n",
    "# Long-term contract\n",
    "profit_long = list()\n",
    "\n",
    "# Repeated contract \n",
    "profit_repeated_IR = list()\n",
    "\n",
    "# Realization contract\n",
    "profit_realized = list()\n",
    "\n",
    "for i in range(len(p_grid)):\n",
    "    e1 = e1_dep[i]\n",
    "    e20 = e20_dep[i]\n",
    "    e21 = e_fb\n",
    "    p = p_grid[i]\n",
    "    ep_sb_p = ep_sb_0[i]\n",
    "    # 1. Long-term solution\n",
    "    profit_long.append(e1*(1-t_1(e1,e20, e21)+e21*(1-t_2(e21)))+(1-e1)*(e20*(1-p-t_2(e20))))\n",
    "    # 2. One-period contract after state is realized\n",
    "    profit_repeated_IR.append(e_sb*(1-c_e(e_sb))+e_sb*(e_sb*(1-c_e(e_sb)))+(1-e_sb)*(ep_sb_p*(1-p-c_e(ep_sb_p))))\n",
    "    # 4. One-period contract after state is realized\n",
    "    profit_realized.append(e_sb*(1-c_e(e_sb))+(e_sb+(1-e_sb)*(1-p))*(e_sb*(1-c_e(e_sb))))\n",
    "    \n",
    "# Plotting the profit to the principal from different types of contracts\n",
    "    \n",
    "plt.figure(figsize=(12,8))\n",
    "plt.style.use('seaborn-white')\n",
    "plt.plot(p_grid, profit_long, label = \"Long-term contract\")\n",
    "plt.plot(p_grid, profit_repeated_IR, label = \"One-period contract with an interim IR\")\n",
    "plt.plot(p_grid, profit_realized, label = \"One-period contract with state realized\")\n",
    "# plt.plot(p_grid, [0.5]*len(p_grid), label = \"e_fb\")\n",
    "# plt.plot(p_grid, [0.25]*len(p_grid), label = \"e_sb\")\n",
    "# plt.plot(p_grid, (1-2*p_grid)/4, label = \"e_sb^p\")\n",
    "# plt.axvline(x=0.245, color='b', ls = '--')\n",
    "plt.xticks([0, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95, 1.00])\n",
    "plt.legend(loc=3, prop={'size': 12})\n",
    "# plt.grid()\n",
    "plt.xlabel('Probability of change in preference (p)', fontname = 'Arial', fontsize = 14)\n",
    "plt.ylabel('Profit for the Principal', fontname='Arial', fontsize = 14)\n",
    "# plt.title('Profit for the Principal (Probability dependent on output)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1ab133",
   "metadata": {},
   "source": [
    "The principal prefers the long-term contract for all values of $p$. The contract where the principal waits for the state to be realized always does better than the one-shot repeated contract with an interim IR. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f2d272",
   "metadata": {},
   "source": [
    "#### 2. Payoff of 1 after failure with probability p after failure in Period 1\n",
    "\n",
    "In this case, $\\alpha = 1$ and $\\beta = 1$. \n",
    "\n",
    "The principal maximizes \n",
    "\n",
    "$$ e_1(1-t_1(1)+e_2(1)(1-t_2(1,1))+(1-e_1)(-t_1(0)+e_2(0)(1-p-t_2(0,1))+p) $$\n",
    "\n",
    "subject to \n",
    "\n",
    "\\begin{align*}\n",
    "    t_1(1) \\geq 0 \\qquad &(LL_1) \\\\\n",
    "    t_1(0) \\geq 0 \\qquad &(LL_0) \\\\\n",
    "    t_1(1)+A(e_2(1)) = a(1) \\qquad & \\\\\n",
    "    t_1(0) + A(e_2(0)) = a(0) \\qquad &\\\\\n",
    "    a(1) - a(0) = c'(e_1) \\qquad &(IC_1) \\\\\n",
    "    t_2(y_1,1) = c'(e_2(y_1)) \\qquad & (IC_2)\n",
    "\\end{align*}\n",
    "\n",
    "The problem reduces to the following: \n",
    "\n",
    "$$ \\max_{e_1, e_2(0), e_2(1), t_1(0)} e_1(1-e_2(1)^2+e_2(1))+(1-e_1)(-e_2(0)^2+e_2(0)(1-p)+p)-2e_1^2-t_1(0)-e_2(0)^2$$\n",
    "\n",
    "subject to \n",
    "\n",
    "$$t_1(0) \\geq 0$$\n",
    "\n",
    "The Lagrangian is then given by: \n",
    "\n",
    "$$L = e_1(1-e_2(1)^2+e_2(1))+(1-e_1)(-e_2(0)^2+e_2(0)(1-p)+p)-2e_1^2-t_1(0)-e_2(0)^2 +\\lambda (t_1(0))$$\n",
    "\n",
    "The FOCs are given by: \n",
    "\n",
    "\\begin{align*}\n",
    "[e_2(1)] :\\ &-2e_2(1)e_1+e_1 = 0 \\implies e_2(1) = \\frac{1}{2} \\\\\n",
    "\\\\\n",
    "[e_2(0)] :\\  & -2(1-e_1)e_2(0)+(1-e_1)(1-p)-2e_2(0) = 0\\\\\n",
    "\\\\\n",
    "[e_1] :\\ & (1+0.25)-4e_1+e_2(0)^2-e_2(0)(1-p)-p = 0 \\\\\n",
    "\\\\\n",
    "[t_1(0)]: \\ &-1+\\lambda = 0 \\implies \\lambda = 1 \\implies t_1(0) = 0\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3e44b0",
   "metadata": {},
   "source": [
    "##### 2 (a). Effort levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226a15b3",
   "metadata": {
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "alpha = 1\n",
    "beta = 1\n",
    "\n",
    "p_grid = np.linspace(0,1,495)\n",
    "\n",
    "e1_dep_f = list()\n",
    "e20_dep_f = list()\n",
    "for i in range(len(p_grid)):\n",
    "    \n",
    "        # No unique solution to the simultaneous equations\n",
    "        # We pick the real solutions between 0 and 1 below\n",
    "        \n",
    "        e1_dep_f.append(abs(f_dep(alpha, beta, p_grid[i])[0][0]))\n",
    "        e20_dep_f.append(abs(f_dep(alpha, beta, p_grid[i])[0][1]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90a95e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(p_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41181a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the effort levels \n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.style.use('seaborn-white')\n",
    "plt.plot(p_grid, e1_dep_f, label = \"e_1D\")\n",
    "plt.plot(p_grid, e20_dep_f, label = \"e_2D(0)\")\n",
    "plt.plot(p_grid, [e_fb]*len(p_grid), label = \"e_2D(1)\")\n",
    "plt.plot(p_grid, [e_sb]*len(p_grid), '--',label = \"Static e_sb\")\n",
    "plt.plot(p_grid, 0.25-(3*p_grid)/16)\n",
    "# plt.plot(p_grid, 0.25*0.25+(0.75)*(1-p_grid)*0.25, label = \"e2_realized\")\n",
    "# plt.axvline(x=0.245, color='b', ls = '--')\n",
    "plt.xticks([0, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90,0.95,1.00])\n",
    "plt.yticks([0, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50])\n",
    "plt.legend(loc=3, prop={'size': 16})\n",
    "plt.xlabel('Probability of change in preference (p)', fontname = 'Arial', fontsize = 14)\n",
    "plt.ylabel('Effort level', fontname='Arial', fontsize = 14)\n",
    "# plt.title('Effort levels vs p (Probability dependent on output)', fontsize = 25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b983867",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(e1_dep_f[247])\n",
    "print(e20_dep_f[247])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bdd1ce",
   "metadata": {},
   "source": [
    "The induced effort level in the second period following a success is the first best and the induced effort level in the second period is decreasing in $p$. The induced effort level in the first period is also decreasing in $p$. As $p$ increases, instead of the principal's gains from winning shrinking after a failure we have that the principal's gains from losing increases. In a static game following a failure, the principal would not want to pay the agent as much even after a success due to this $p$ and as a result the wedge between $t_1$ and $t_0$ would shrink providing lower incentives. If the outcome in the first period is a failure and $p=1$, the princpal is indifferent between success and failure and would pay $t_2(0) = 0$ and have the agent exert no effort. The difference here is that after a failure the principal actually has something to gain from losing instead of having something to lose from winning. If the first-period outcome is a success then the principal gets a payoff of 1 with probability $e_2(1)$. However, if the first-period outcome is a failure then the principal gets a payoff of 1 with probability $e_2(0)+(1-e_2(0))p$. The larger the $p$ the bigger the expected payoff in the second-period from failing in the first-period. Therefore, as $p$ increases principal's desire to lose in the first period increases and so effort in the first period decreases.  \n",
    "\n",
    "Let us consider a team with $p=1$. This team knows that if they lose in the second period following a failure they are going to get a draft pick for certain. If they lose in the first period but win in the second period, they will still get the winning bonus. As a result, they would want to lose in the first-period. Why provide any incentive in the first-period at all? This is to get the winning bonus in the first-period **(see analysis with discount factor below)**.  \n",
    "\n",
    "This also demonstrates a 'vicious' and 'virtuous' cycle. Teams with smaller $p$ values induce higher effort in the first period and therefore less likely to experience this change in preference at all. Teams with larger $p$ values induce smaller effort in the first period and therefore more likely to experience this change in preference and want to lose in the second period as well. \n",
    "\n",
    "Why is the assumption realistic? Think of bonus from winning and chances at a better draft pick from losing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38202d85",
   "metadata": {},
   "source": [
    "##### 2 (b). Rent to the agent \n",
    "\n",
    "We will compare the rent to the agent when 1) long-term contract is offered, 2) one-shot contract with an interim IR is offered, 3) one-period contract with no interim IR is offered and 4) one-period contract after the state is realized is offered. \n",
    "\n",
    "The one-period contract after the state is realized is going to implement second-best in the first-period. After a success in the first-period, the principal will implement $e^{SB}$ in the second-period as well and this occurs with a probability of $e_1$. After a failure, the principal will implement $e^{SB}$ if there is no change in preference and 0 otherwise. \n",
    "\n",
    "The one-shot contract with an interim IR will implement $e^{SB}$ in the first-period. In the second period, the principal will implement $e^{SB}$ after a success. After a failure, the principal will implement $e_p^{SB}$ in the second period. \n",
    "1. Long-term Contract: $A(e_1)+A(e_2(0))$\n",
    "2. One-shot contract with an interim IR: $A(e^{SB})+e^{SB}A(e^{SB})+(1-e^{SB})A(e^{SB}_p)$\n",
    "3. One-shot contract without an interim IR: \n",
    "4. One-period contract after the state is realized: $A(e^{SB})+(e^{SB}+(1-e^{SB})(1-p))A(e^{SB})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f82175c",
   "metadata": {
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "# Comparison of rents \n",
    "\n",
    "# 1. Long-term solution rent \n",
    "long_grid = list()\n",
    "\n",
    "# 2. One-period contract contract with interim IR\n",
    "repeated_IR_grid = list()\n",
    "\n",
    "# 4. One-period contract after the state is realized\n",
    "realized_grid = list()\n",
    "\n",
    "e_sb = 0.25 \n",
    "\n",
    "for i in range(len(p_grid)):\n",
    "    e1= e1_dep_f[i]\n",
    "    e20 = e20_dep_f[i]\n",
    "    e21 = e_fb\n",
    "    p = p_grid[i]\n",
    "    e_sb_p = ep_sb[i]\n",
    "    # 1. Long-term solution \n",
    "    long_grid.append(A(e1)+A(e20))\n",
    "    # 2. One-period contract with interim IR\n",
    "    repeated_IR_grid.append(A(e_sb)+e_sb*A(e_sb)+(1-e_sb)*A(e_sb_p))\n",
    "    # 4. One-period contract after state is realized\n",
    "    realized_grid.append(A(e_sb)+(1-p*(1-e_sb))*A(e_sb))\n",
    "    \n",
    "# Plotting the rents \n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(p_grid, long_grid, label = \"Optimal long-term contract\")\n",
    "plt.plot(p_grid, repeated_IR_grid, label = \"One-period contracts with an interim IR\")\n",
    "plt.plot(p_grid, realized_grid, label = \"One-period contracts with observed state\")\n",
    "# plt.plot(p_grid, [0.5]*len(p_grid), label = \"e_fb\")\n",
    "# plt.plot(p_grid, [0.25]*len(p_grid), label = \"e_sb\")\n",
    "# plt.plot(p_grid, (1-2*p_grid)/4, label = \"e_sb^p\")\n",
    "# plt.axvline(x=0.245, color='b', ls = '--')\n",
    "plt.xticks([0, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95, 1.00])\n",
    "# plt.yticks([0, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50])\n",
    "plt.legend(loc=3, prop={'size': 16})\n",
    "# plt.grid()\n",
    "plt.xlabel('Probability of change in preference (p)', fontname = 'Arial', fontsize = 14)\n",
    "plt.ylabel('Rent to the agent', fontname='Arial', fontsize = 14)\n",
    "# plt.title('Rent to the agent(Probability dependent on output)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c050d676",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(long_grid[247])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739eb7d9",
   "metadata": {},
   "source": [
    "The agent prefers the contract where the principal waits for the state to be realized for all values of $p$. The agent is worse of in the long-term contract for all values of $p$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efebbbcc",
   "metadata": {
    "hideCode": false
   },
   "source": [
    "##### 2(c). Profit to the principal \n",
    "\n",
    "We will compare the profit to the principal when 1) long-term contract is offered, 2) one-shot contract with an interim IR is offered, 3) one-period contract with no interim IR is offered and 4) one-period contract after the state is realized is offered. \n",
    "\n",
    "\n",
    "1. Long-term Contract: $$e_1(1-t_1(1)+e_2(1)(1-t_2(1,1)))+(1-e_1)(e_2(0)(1-t_2(0,1))+(1-e_2(0))p)$$\n",
    "\n",
    "2. One-shot contract with an interim IR: $$e^{SB}(1-c'(e^{SB}))+e^{SB}(e^{SB}(1-c'(e^{SB})))+(1-e^{SB})(e_p^{SB}(1-c'(e_p^{SB}))+(1-e_p^{SB})p)$$\n",
    "\n",
    "3. One-shot contract without an interim IR: \n",
    "\n",
    "4. One-period contract after the state is realized: $$e^{SB}(1-c'(e^{SB}))+(e^{SB}+(1-e^{SB})(1-p))(e^{SB}(1-c'(e^{SB})))+(1-e^{SB})p$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3cb2ca",
   "metadata": {
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "# Comparison of profits \n",
    "\n",
    "# Long-term contract\n",
    "profit_long = list()\n",
    "\n",
    "# Repeated contract \n",
    "profit_repeated_IR = list()\n",
    "\n",
    "# Realization contract\n",
    "profit_realized = list()\n",
    "\n",
    "for i in range(len(p_grid)):\n",
    "    e1 = e1_dep_f[i]\n",
    "    e20 = e20_dep_f[i]\n",
    "    e21 = e_fb\n",
    "    p = p_grid[i]\n",
    "    ep_sb_p = ep_sb_0[i]\n",
    "    # 1. Long-term solution\n",
    "    profit_long.append(e1*(1-t_1(e1,e20, e21)+e21*(1-t_2(e21)))+(1-e1)*(e20*(1-t_2(e20))+(1-e20)*p))\n",
    "    # 2. One-period contract after state is realized\n",
    "    profit_repeated_IR.append(e_sb*(1-c_e(e_sb))+e_sb*(e_sb*(1-c_e(e_sb)))+(1-e_sb)*(ep_sb_p*(1-c_e(ep_sb_p))+(1-ep_sb_p)*p))\n",
    "    # 4. One-period contract after state is realized\n",
    "    profit_realized.append(e_sb*(1-c_e(e_sb))+(e_sb+(1-e_sb)*(1-p))*(e_sb*(1-c_e(e_sb)))+(1-e_sb)*p)\n",
    "    \n",
    "    \n",
    "# Plotting the profit to the principal from different types of contract\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(p_grid, profit_long, label = \"Optimal long-term contract\")\n",
    "plt.plot(p_grid, profit_repeated_IR, label = \"One-period contracts with an interim IR\")\n",
    "plt.plot(p_grid, profit_realized, label = \"One-period contracts with observed state\")\n",
    "# plt.plot(p_grid, [0.5]*len(p_grid), label = \"e_fb\")\n",
    "# plt.plot(p_grid, [0.25]*len(p_grid), label = \"e_sb\")\n",
    "# plt.plot(p_grid, (1-2*p_grid)/4, label = \"e_sb^p\")\n",
    "# plt.axvline(x=0.245, color='b', ls = '--')\n",
    "plt.xticks([0, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95, 1.00])\n",
    "plt.legend(loc=2, prop={'size': 16})\n",
    "# plt.grid()\n",
    "plt.xlabel('Probability of change in preference (p)', fontname = 'Arial', fontsize = 14)\n",
    "plt.ylabel('Profit for the principal', fontname='Arial', fontsize = 14)\n",
    "# plt.title('Rent to the agent(Probability dependent on output)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca353a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(profit_repeated_IR[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31d3cb7",
   "metadata": {},
   "source": [
    "The principal prefers the long-term contract to all other contracts for all values of $p$. The contract where the principal waits for the state to be realized is better for the principal than the one-shot contract with an interim IR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6741bbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'p_grid_2': p_grid_2,\n",
    "                   'e1_grid': e1_grid,\n",
    "                 'e_fb': [e_fb]*len(p_grid_2)}\n",
    "type(df) \n",
    "# sns.lineplot(data = df, x = df['p_grid_2'], y = df['e1_grid'])\n",
    "\n",
    "sns.lmplot(x='p_grid_2', y='value', hue='variable', data=df, fit_reg=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ced7ed8",
   "metadata": {},
   "source": [
    "##### 2(d). Adding discounting to check if first-period effort changes\n",
    "\n",
    "\n",
    "In this case, $\\alpha = 1$ and $\\beta = 1$. Let $\\delta$ be the discount factor.  \n",
    "\n",
    "The principal maximizes \n",
    "\n",
    "$$ e_1(1-t_1(1)+\\delta(e_2(1)(1-t_2(1,1)))+(1-e_1)(-t_1(0)+\\delta(e_2(0)(1-p-t_2(0,1))+p)) $$\n",
    "\n",
    "subject to \n",
    "\n",
    "\\begin{align*}\n",
    "    t_1(1) \\geq 0 \\qquad &(LL_1) \\\\\n",
    "    t_1(0) \\geq 0 \\qquad &(LL_0) \\\\\n",
    "    t_1(1)+\\delta A(e_2(1)) = a(1) \\qquad & \\\\\n",
    "    t_1(0) + \\delta A(e_2(0)) = a(0) \\qquad &\\\\\n",
    "    a(1) - a(0) = c'(e_1) \\qquad &(IC_1) \\\\\n",
    "    t_2(y_1,1) = c'(e_2(y_1)) \\qquad & (IC_2)\n",
    "\\end{align*}\n",
    "\n",
    "where $a(y_1) = t_1(y_1)+\\delta(e_2(y_1)t_2(y_1,1)-c(e_2(y_1))$ for $y_1 \\in \\{0,1\\}$. \n",
    "\n",
    "The problem reduces to the following: \n",
    "\n",
    "$$ \\max_{e_1, e_2(0), e_2(1), t_1(0)} e_1(1-\\delta e_2(1)^2+\\delta e_2(1))+(1-e_1)(-\\delta e_2(0)^2+\\delta e_2(0)(1-p)+\\delta p)-2e_1^2-t_1(0)-\\delta e_2(0)^2$$\n",
    "\n",
    "subject to \n",
    "\n",
    "$$t_1(0) \\geq 0$$\n",
    "\n",
    "The Lagrangian is then given by: \n",
    "\n",
    "$$L = e_1(1-\\delta e_2(1)^2+\\delta e_2(1))+(1-e_1)(-\\delta e_2(0)^2+\\delta e_2(0)(1-p)+\\delta p)-2e_1^2-t_1(0)-\\delta e_2(0)^2 +\\lambda (t_1(0))$$\n",
    "\n",
    "The FOCs are given by: \n",
    "\n",
    "\\begin{align*}\n",
    "[e_2(1)] :\\ &-\\delta 2e_2(1)e_1+\\delta e_1 = 0 \\implies e_2(1) = \\frac{1}{2} \\\\\n",
    "\\\\\n",
    "[e_2(0)] :\\  & \\delta(-2(1-e_1)e_2(0)+(1-e_1)(1-p)-2e_2(0)) = 0 \\implies -2(1-e_1)e_2(0)+(1-e_1)(1-p)-2e_2(0) = 0 \\\\\n",
    "\\\\\n",
    "[e_1] :\\ & -4e_1+1+\\delta(0.25+e_2(0)^2-e_2(0)(1-p)-p) = 0 \\\\\n",
    "\\\\\n",
    "[t_1(0)]: \\ &-1+\\lambda = 0 \\implies \\lambda = 1 \\implies t_1(0) = 0\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461f9a09",
   "metadata": {
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "# Solves for the effort level given a value of p and discount factor delta\n",
    "\n",
    "def f_delta(p, delta):\n",
    "    \n",
    "    # solves for effort levels e_1 and e_20 as a function of p and delta\n",
    "    # a = e_1, b = e_20\n",
    "    \n",
    "    var('a b p')    \n",
    "    eqns = [1+delta*(0.25-b*(1-p)+b**2-p)-4*a, (1-a)*(1-p)-(1-a)*2*b-2*b]\n",
    "    soln = solve(eqns, [a,b])\n",
    "    return soln"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5f6ea5",
   "metadata": {},
   "source": [
    "##### 2 (d) (i). Effort levels  ($\\delta$ = 0.5 and $\\delta$ = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba4d8f1",
   "metadata": {
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Delta = 0.5 \n",
    "\n",
    "e1_delta = list()\n",
    "e20_delta = list()\n",
    "for i in range(len(p_grid)):\n",
    "    \n",
    "        # No unique solution to the simultaneous equations\n",
    "        # We pick the real solutions between 0 and 1 below\n",
    "        \n",
    "        e1_delta.append(abs(f_delta(p_grid[i], 0.5)[0][0]))\n",
    "        e20_delta.append(abs(f_delta(p_grid[i],0.5)[0][1]))\n",
    "        \n",
    "\n",
    "# Delta = 0.25 \n",
    "\n",
    "e1_delta_1 = list()\n",
    "e20_delta_1 = list()\n",
    "for i in range(len(p_grid)):\n",
    "    \n",
    "        # No unique solution to the simultaneous equations\n",
    "        # We pick the real solutions between 0 and 1 below\n",
    "        \n",
    "        e1_delta_1.append(abs(f_delta(p_grid[i], 0.25)[0][0]))\n",
    "        e20_delta_1.append(abs(f_delta(p_grid[i],0.25)[0][1]))\n",
    "        \n",
    "# Plotting the effort levels \n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.plot(p_grid, e1_delta, label = \"e_1_0.5\")\n",
    "plt.plot(p_grid, e1_delta_1, label = \"e_1_0.25\")\n",
    "# plt.plot(p_grid, e20_delta, label = \"e_20_0.5\")\n",
    "plt.plot(p_grid, e20_delta_1, label = \"e_20_0.25\")\n",
    "plt.plot(p_grid, [e_fb]*len(p_grid), label = \"e_21\")\n",
    "plt.plot(p_grid, [e_sb]*len(p_grid), label = \"e_sb\")\n",
    "# plt.plot(p_grid, 0.25*0.25+(0.75)*(1-p_grid)*0.25, label = \"e2_realized\")\n",
    "# plt.axvline(x=0.245, color='b', ls = '--')\n",
    "plt.xticks([0, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90,0.95,1.00])\n",
    "plt.yticks([0, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50])\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f836aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(e20_delta)\n",
    "print(e20_delta_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6457ea05",
   "metadata": {},
   "source": [
    "The above analysis was done to see why the principal induces any effort at all in the first period when the principal receives a payoff of 1 with probability $p=1$. The question is important because in such a scenario the principal receives a payoff of 1 for sure in the second period after a first-period failure. The hypothesis was that the principal induces first-period effort to receive the first-period payoff. We can see that this hypothesis is indeed correct as first period effort increases as discount factor decreases. The less the principal values the future the higher the induced first-period effort level is. Therefore, the principal only induces first-period effort level in this case to receive the first-period payoff. There is no need to induce first-period effort to affect second-period payoffs in this case with $p=1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeb1314",
   "metadata": {},
   "source": [
    "#### 3. Payoff of 1 after failure and 0 after success with probability p after failure in Period 1\n",
    "\n",
    "In this case, $\\alpha = 1$ and $\\beta = 0$. \n",
    "\n",
    "The principal maximizes \n",
    "\n",
    "$$ e_1(1-t_1(1)+e_2(1)(1-t_2(1,1))+(1-e_1)(-t_1(0)+e_2(0)(1-2p-t_2(0,1))+p) $$\n",
    "\n",
    "subject to \n",
    "\n",
    "\\begin{align*}\n",
    "    t_1(1) \\geq 0 \\qquad &(LL_1) \\\\\n",
    "    t_1(0) \\geq 0 \\qquad &(LL_0) \\\\\n",
    "    t_1(1)+A(e_2(1)) = a(1) \\qquad & \\\\\n",
    "    t_1(0) + A(e_2(0)) = a(0) \\qquad &\\\\\n",
    "    a(1) - a(0) = c'(e_1) \\qquad &(IC_1) \\\\\n",
    "    t_2(y_1,1) = c'(e_2(y_1)) \\qquad & (IC_2)\n",
    "\\end{align*}\n",
    "\n",
    "The problem reduces to the following: \n",
    "\n",
    "$$ \\max_{e_1, e_2(0), e_2(1), t_1(0)} e_1(1-e_2(1)^2+e_2(1))+(1-e_1)(-e_2(0)^2+e_2(0)(1-2p)+p)-2e_1^2-t_1(0)-e_2(0)^2$$\n",
    "\n",
    "subject to \n",
    "\n",
    "$$t_1(0) \\geq 0$$\n",
    "\n",
    "The Lagrangian is then given by: \n",
    "\n",
    "$$L = e_1(1-e_2(1)^2+e_2(1))+(1-e_1)(-e_2(0)^2+e_2(0)(1-2p)+p)-2e_1^2-t_1(0)-e_2(0)^2 +\\lambda (t_1(0))$$\n",
    "\n",
    "The FOCs are given by: \n",
    "\n",
    "\\begin{align*}\n",
    "[e_2(1)] :\\ &-2e_2(1)e_1+e_1 = 0 \\implies e_2(1) = \\frac{1}{2} \\\\\n",
    "\\\\\n",
    "[e_2(0)] :\\  & -2(1-e_1)e_2(0)+(1-e_1)(1-2p)-2e_2(0) = 0\\\\\n",
    "\\\\\n",
    "[e_1] :\\ & (1+0.25)-4e_1+e_2(0)^2-e_2(0)(1-2p)-p = 0 \\\\\n",
    "\\\\\n",
    "[t_1(0)]: \\ &-1+\\lambda = 0 \\implies \\lambda = 1 \\implies t_1(0) = 0\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819e171d",
   "metadata": {},
   "source": [
    "##### 3 (a). Effort levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b0862b",
   "metadata": {
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "alpha = 1\n",
    "beta = 0\n",
    "\n",
    "e1_dep = list()\n",
    "e20_dep_p = list()\n",
    "for i in range(len(p_grid_2)):\n",
    "    \n",
    "        # No unique solution to the simultaneous equations\n",
    "        # We pick the real solutions between 0 and 1 below\n",
    "        \n",
    "        e1_dep.append(abs(f_dep(alpha, beta, p_grid_2[i])[0][0]))\n",
    "        e20_dep_p.append(abs(f_dep(alpha, beta, p_grid_2[i])[0][1]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c8d7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the effort levels \n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(p_grid_2, e1_dep, label = \"e1_new\")\n",
    "plt.plot(p_grid_2, e20_dep_p, label = \"e_20\")\n",
    "plt.plot(p_grid[0:247], e1_dep_f[0:247], label = \"e1_old\")\n",
    "plt.plot(p_grid_2, [e_fb]*len(p_grid_2), label = \"e_21\")\n",
    "plt.plot(p_grid_2, [e_sb]*len(p_grid_2), label = \"e_sb\")\n",
    "# plt.plot(p_grid, 0.25*0.25+(0.75)*(1-p_grid)*0.25, label = \"e2_realized\")\n",
    "# plt.axvline(x=0.245, color='b', ls = '--')\n",
    "plt.xticks([0, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50])\n",
    "plt.legend(loc=3, prop={'size': 16})\n",
    "plt.xlabel('Probability of change in preference (p)', fontname = 'Arial', fontsize = 14)\n",
    "plt.ylabel('Effort level', fontname = 'Arial', fontsize = 14)\n",
    "# plt.title('Effort levels vs p (Probability dependent on output)', fontsize = 25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a3ae46",
   "metadata": {},
   "source": [
    "The effort in the first-period is decreasing non-linearly as compared to the case where the principal receives a payoff of 1 after failure with probability p. The effort in the first period also does not decrease as much as that case. To figure out why, let us consider the extreme case of $p=1$. After a success in the first-period, the principal receives a payoff of 1 in the second-period with probability $e_2(1)$. After a failure in the first period, the principal receives 1 for sure after a failure in the second period and 0 for sure after a success in the second period. Therefore, after a failure in the first period the principal receives a payoff of 1 in the second period with probability $1-e_2(0)$. As a result, first-period effort is still important because you are not guaranteed a payoff of 1 in the second period after a failure in the first-period as in the case where the principal receives a payoff of 1 after failure with probability $p=1$. In that case, you are guaranteed a payoff of 1 after failing in the first period so incentives to induce shirking is higher. In this particular case, payoff of 1 is not guaranteed after failing therefore the incentives to induce shirking in the first-period is lower which is reflected in the higher first-period effort levels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9216ad4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(e1_dep_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929d8277",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(e1_dep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a030a874",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.25\n",
    "beta = 0.25\n",
    "\n",
    "e1_dep = list()\n",
    "e20_dep_p = list()\n",
    "for i in range(len(p_grid_2)):\n",
    "    \n",
    "        # No unique solution to the simultaneous equations\n",
    "        # We pick the real solutions between 0 and 1 below\n",
    "        \n",
    "        e1_dep.append(abs(f_dep(alpha, beta, p_grid_2[i])[0][0]))\n",
    "        e20_dep_p.append(abs(f_dep(alpha, beta, p_grid_2[i])[0][1]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39648f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.plot(p_grid_2, e1_dep, label = \"e_1'\")\n",
    "plt.plot(p_grid_2, e20_dep_p, label = \"e_20\")\n",
    "plt.plot(p_grid[0:13], e1_dep_f[0:13], label = \"e_1\")\n",
    "plt.plot(p_grid_2, [e_fb]*len(p_grid_2), label = \"e_21\")\n",
    "plt.plot(p_grid_2, [e_sb]*len(p_grid_2), label = \"e_sb\")\n",
    "# plt.plot(p_grid, 0.25*0.25+(0.75)*(1-p_grid)*0.25, label = \"e2_realized\")\n",
    "# plt.axvline(x=0.245, color='b', ls = '--')\n",
    "plt.xticks([0, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50])\n",
    "plt.yticks([0, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50])\n",
    "plt.legend(loc=3, prop={'size': 25})\n",
    "plt.xlabel('Probability of change in preference (p)', fontsize = 20)\n",
    "plt.ylabel('Effort level', fontsize = 20)\n",
    "plt.title('Effort levels vs p (Probability dependent on output)', fontsize = 25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177c1f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = []\n",
    "for i in range(len(e20_dep_p)):\n",
    "    e20 = e20_dep_p[i]\n",
    "    p = p_grid_2[i]\n",
    "    val.append(1+e20**2-e20+p*e20-p)\n",
    "plt.plot(p_grid_2, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6ae56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_grid = np.linspace(0, 1, 11)\n",
    "x_grid = np.linspace(0, 1, 11)\n",
    "g_list = np.zeros((len(x_grid), len(y_grid)))\n",
    "\n",
    "for i in range(len(x_grid)):\n",
    "    x = x_grid[i]\n",
    "    for j in range(len(y_grid)):\n",
    "        y = y_grid[j]\n",
    "        g = -6*y**2+6*(1-x)*y+13.5+2*x-(1-x)**2\n",
    "        g_list[i,j] = g\n",
    "    \n",
    "plt.figure(figsize=(15,15))\n",
    "h = plt.contourf(x_grid,y_grid, g_list)\n",
    "plt.axvline(x = 0.5, color = 'b', label = 'axvline - full height')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.axis('scaled')\n",
    "plt.colorbar()\n",
    "plt.xlabel('p', fontsize = 20)\n",
    "plt.ylabel('e_2d', fontsize = 20)\n",
    "plt.title('Effort levels in the first-period', fontsize = 25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcc9d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = []\n",
    "for i in range(len(x_grid)):\n",
    "    p = x_grid[i]\n",
    "    gd = g_list[i, :5]\n",
    "    print(gd)\n",
    "    gs = np.argmax(f)\n",
    "    print(g)\n",
    "    val.append(y_grid[gs])\n",
    "print(val)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefe858d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_grid = np.linspace(0, 1, 11)\n",
    "x_grid = np.linspace(0, 1, 11)\n",
    "f_list = np.zeros((len(x_grid), len(y_grid)))\n",
    "\n",
    "for i in range(len(x_grid)):\n",
    "    x = x_grid[i]\n",
    "    for j in range(len(y_grid)):\n",
    "        y = y_grid[j]\n",
    "        f = 3*y**2-2*y-2*(1-x)*y-2*x-(7/4)\n",
    "        f_list[i,j] = f\n",
    "        \n",
    "plt.figure(figsize=(15,15))\n",
    "h = plt.contourf(x_grid,y_grid, f_list)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.axis('scaled')\n",
    "plt.colorbar()\n",
    "plt.xlabel('p', fontsize = 20)\n",
    "plt.ylabel('e_2d', fontsize = 20)\n",
    "plt.title('Effort levels in the first-period', fontsize = 25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fff178",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = []\n",
    "for i in range(len(x_grid)):\n",
    "    p = x_grid[i]\n",
    "    f = f_list[i, :5]\n",
    "    print(f)\n",
    "    g = np.argmax(f)\n",
    "    print(g)\n",
    "    val.append(y_grid[g])\n",
    "print(val)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94080b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "grd = np.zeros((len(x_grid), len(y_grid)))\n",
    "for i in range(len(x_grid)):\n",
    "    for j in range(len(y_grid)):\n",
    "        grd[i,j] = f_list[i,j]/g_list[i,j]\n",
    "    val = grd[i,:5]\n",
    "    print(val)\n",
    "print(np.min(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc99127",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_grid = np.linspace(0,1,11)\n",
    "y_grid = np.linspace(0,1,11)\n",
    "\n",
    "glz = np.zeros((len(x_grid), len(y_grid)))\n",
    "\n",
    "for i in range(len(x_grid)):\n",
    "    x = x_grid[i]\n",
    "    for j in range(len(y_grid)):\n",
    "        y = y_grid[j]\n",
    "        glz[i,j]=2*y-(1-x)\n",
    "        \n",
    "print(glz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd78040",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d514a88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Hide code",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
